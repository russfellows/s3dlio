# s3dlio Outstanding Work Items

**Date**: October 9, 2025  
**Current Version**: v0.9.4  
**Status**: API deprecation phase, universal backend support complete

---

## Overview

This document tracks outstanding work items for s3dlio. Major features are stable, with focus on polish, testing, and preparing for v1.0.0 release.

---

## ✅ Recently Completed (v0.9.3 - v0.9.4)

### v0.9.4: API Deprecation & Universal Backend Support
- ✅ Deprecated `list_objects()` in Rust (`src/s3_utils.rs`) - use `ObjectStore::list()` instead
- ✅ Deprecated `list_objects()` in Python - use `s3dlio.list()` instead
- ✅ Deprecated `get_object()` in Python - use `s3dlio.get()` or `s3dlio.get_range()` instead
- ✅ CLI improvements: Alphabetical command ordering, clear deprecation notices
- ✅ Verified all 5 backends have RangeEngine concurrent downloads (30-50% faster)
- ✅ Created comprehensive backend documentation (`v0.9.4_BackEnd_Range-Summary.md`)
- ✅ Kept `create_bucket()` and `delete_bucket()` active (will make universal in future)
- ✅ Documentation cleanup: 60+ docs → 15 essential docs + organized archive

### v0.9.3: Generic RangeEngine Implementation
- ✅ Created `src/range_engine_generic.rs` (492 lines, 4 tests)
- ✅ Universal stream-based architecture for Azure, GCS, File, DirectIO backends
- ✅ Integrated into all non-S3 backends (S3 uses custom optimized implementation)
- ✅ All tests passing (16/16 tests)
- ✅ Constants consolidated in `src/constants.rs`

### v0.9.2: CancellationToken Infrastructure
- ✅ Graceful shutdown for all DataLoader components
- ✅ Updated LoaderOptions, spawn_prefetch(), DataLoader, AsyncPoolDataLoader
- ✅ 9 comprehensive cancellation tests (all passing)
- ✅ Documentation: `CANCELLATION-TOKEN-IMPLEMENTATION.md` (archived)

### v0.9.0 - v0.9.1: Zero-Copy & Multi-Backend
- ✅ Zero-copy API for high-performance data access
- ✅ Azure Blob Storage backend fully functional
- ✅ GCS backend fully functional (pagination bug fixed in v0.8.22)
- ✅ Configuration hierarchy system
- ✅ OpLog shared crate (`s3dlio-oplog`) for replay functionality

---

## 🚧 In Progress (v0.9.4+)

### 1. Deprecation Warnings → Removal (Target: v1.0.0)
**Priority:** HIGH (breaking change for v1.0.0)  
**Effort:** Low

Deprecated functions will be removed in v1.0.0:
- `list_objects()` - Use `ObjectStore::list()` (Rust) or `s3dlio.list()` (Python)
- `get_object()` - Use `s3dlio.get()` or `s3dlio.get_range()` (Python only)

**Timeline:**
- v0.9.4: Deprecation warnings active ✅
- v0.9.5 - v0.9.x: Additional warnings and migration support
- v1.0.0: Remove deprecated functions

**Action Items:**
- [ ] Add migration examples to documentation
- [ ] Scan dl-driver and s3-bench for usage of deprecated functions
- [ ] Create automated migration script/tool

---

## 📋 Planned Enhancements (Post-v0.9.4)

### 2. Universal Bucket Operations
**Priority:** MEDIUM  
**Effort:** Medium  
**Target:** v0.9.5 or v1.0.0

Make `create_bucket()` and `delete_bucket()` work across all backends:

**Current Status:**
- Functions exist but only tested with S3
- Need to verify Azure container operations
- Need to verify GCS bucket operations
- Local filesystem: create/delete directories
- DirectIO: create/delete directories

**Tasks:**
- [ ] Implement `create_bucket()` for Azure (`az://container`)
- [ ] Implement `create_bucket()` for GCS (`gs://bucket`)
- [ ] Implement `create_bucket()` for file:// (create directory)
- [ ] Implement `create_bucket()` for direct:// (create directory)
- [ ] Add universal tests for all backends
- [ ] Update CLI `create-bucket` and `delete-bucket` commands
- [ ] Update Python API documentation

---

### 3. GCS Python API Testing
**Priority:** MEDIUM  
**Effort:** Low

**Status:** Rust GCS backend complete and tested, Python bindings untested

**What needs testing:**
- [ ] `s3dlio.upload(['file.txt'], 'gs://bucket/prefix/')`
- [ ] `s3dlio.download('gs://bucket/*.txt', './local/')`
- [ ] `S3IterableDataset("gs://bucket/data/", ...)`
- [ ] PyTorch DataLoader integration with GCS
- [ ] TensorFlow dataset integration with GCS
- [ ] Error handling in Python layer

**How to test:**
```bash
# Rebuild Python extension
./build_pyo3.sh && ./install_pyo3_wheel.sh

# Create test file
cat > tests/test_gcs_api.py << 'EOF'
import s3dlio
import pytest

def test_gcs_upload():
    s3dlio.upload(['test.txt'], 'gs://test-bucket/prefix/')

def test_gcs_download():
    s3dlio.download('gs://test-bucket/prefix/*.txt', './gcs_local/')

# ... more tests
EOF

# Run tests
python tests/test_gcs_api.py
```

---

### 4. Performance Benchmarking Suite
**Priority:** LOW  
**Effort:** Medium

**Goal:** Comprehensive performance comparison across all backends

**Current State:**
- `scripts/run_backend_comparison.sh` exists but only compares S3 vs Arrow backends
- No systematic benchmarking of Azure, GCS, File, DirectIO

**Tasks:**
- [ ] Extend `run_backend_comparison.sh` to include all 5 backends
- [ ] Add throughput benchmarks (GET/PUT)
- [ ] Add latency benchmarks (small objects)
- [ ] Add concurrency scaling tests (1-64 concurrent operations)
- [ ] Add large file tests (1GB+, RangeEngine effectiveness)
- [ ] Generate performance report with graphs
- [ ] Document performance tuning recommendations per backend

**Example Output:**
```
Backend Performance Comparison (100MB file, 16 concurrent ops)
----------------------------------------------------------------
S3 (MinIO):        GET: 5.2 GB/s   PUT: 2.8 GB/s
Azure Blob:        GET: 4.8 GB/s   PUT: 2.5 GB/s
GCS:               GET: 4.9 GB/s   PUT: 2.6 GB/s
File (NVMe):       GET: 12.0 GB/s  PUT: 8.0 GB/s
DirectIO (NVMe):   GET: 15.0 GB/s  PUT: 10.0 GB/s
```

---

### 5. Documentation Improvements
**Priority:** MEDIUM  
**Effort:** Low

**Completed:**
- ✅ Organized docs/ directory (60+ → 15 essential)
- ✅ Created archive/ for historical documents
- ✅ Created `BACKEND-TESTING.md` (consolidated testing guide)
- ✅ Created `OPLOG-GUIDE.md` (consolidated oplog documentation)
- ✅ Created `v0.9.4_BackEnd_Range-Summary.md` (RangeEngine analysis)

**Remaining Tasks:**
- [ ] Create migration guide for v0.9.x → v1.0.0
- [ ] Update README.md with v0.9.4 features
- [ ] Add "Getting Started" tutorial for each backend
- [ ] Add performance tuning guide per backend
- [ ] Create troubleshooting guide with common issues

---

### 6. Multi-Target S3 Addressing (>100 Gb/s)
**Priority:** LOW (future enhancement)  
**Effort:** HIGH

**Current Limitation:**
- Single S3 endpoint per process
- Multi-target scaling handled at process level (different instances → different IPs)
- Current max: ~100 Gb/s per process (bonded 100 Gb ports)

**Enhancement Goal:**
- Support multiple S3 endpoints within single s3dlio instance
- Round-robin or load-balanced distribution
- Target: >100 Gb/s throughput for large deployments

**Use Case:**
- Vast storage with multiple bonded 100 Gb ports
- Each port = different IP address
- Aggregate bandwidth > 100 Gb/s

**Design Considerations:**
- [ ] Multi-endpoint configuration in `S3Config`
- [ ] Round-robin or hash-based endpoint selection
- [ ] Connection pool per endpoint
- [ ] Health checking and failover
- [ ] Benchmarking infrastructure for multi-endpoint setups

**Status:** Deferred until v1.1.0+ (current single-endpoint approach works well)

---

## 🐛 Known Issues

### None Currently
All known issues from v0.9.0-v0.9.3 have been resolved.

**Recently Fixed:**
- ✅ GCS pagination (v0.8.22)
- ✅ Azure range downloads (v0.9.1)
- ✅ DirectIO alignment (v0.9.0)
- ✅ Cancellation token edge cases (v0.9.2)
- ✅ Configuration hierarchy conflicts (v0.9.2)

---

## 📊 Testing Status

### Backend Coverage

| Feature | S3 | Azure | GCS | File | DirectIO |
|---------|-----|-------|-----|------|----------|
| List (< 1k objects) | ✅ | ✅ | ✅ | ✅ | ✅ |
| List (> 1k objects) | ✅ | ✅ | ✅ | ✅ | ✅ |
| Range downloads | ✅ | ✅ | ✅ | ✅ | ✅ |
| Concurrent uploads | ✅ | ✅ | ✅ | ✅ | ✅ |
| Special characters | ✅ | ✅ | ✅ | ✅ | ✅ |
| Python API | ✅ | ✅ | ⚠️ | ✅ | ✅ |
| PyTorch DataLoader | ✅ | ✅ | ⚠️ | ✅ | ✅ |
| Create/Delete Bucket | ✅ | ❓ | ❓ | ✅ | ✅ |

**Legend:** ✅ Tested | ⚠️ Needs testing | ❓ Not verified | ❌ Not supported

---

## 🎯 v1.0.0 Release Criteria

**Target Date:** TBD (after v0.9.5+)

**Must-Have:**
- ✅ All 5 backends production-ready
- ✅ RangeEngine working across all backends
- ✅ Zero compiler warnings
- ✅ Comprehensive documentation
- [ ] Remove deprecated functions (`list_objects`, `get_object`)
- [ ] Universal bucket operations (create/delete)
- [ ] GCS Python API fully tested
- [ ] Migration guide for v0.9.x → v1.0.0
- [ ] Performance benchmarking complete
- [ ] Stability testing (no regressions for 2+ weeks)

**Nice-to-Have:**
- Performance optimization beyond current targets
- Additional backend support (e.g., MinIO-specific features)
- Enhanced monitoring and observability
- Multi-target S3 addressing

---

## 📚 See Also

- **Changelog.md** - Detailed release notes
- **RELEASE-CHECKLIST.md** - Release process
- **BACKEND-TESTING.md** - Backend testing guide
- **OPLOG-GUIDE.md** - Operation logging guide
- **v0.9.4_BackEnd_Range-Summary.md** - RangeEngine analysis
- **DEPRECATION-NOTICE-v0.9.4.md** - Current deprecations
- **docs/archive/** - Historical planning documents
