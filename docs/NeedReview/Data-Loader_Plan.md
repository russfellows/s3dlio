# Data Loader Plan
## Prompt
I want to create a "data loader" function for my S3 library.  I have a github project called s3dlio that implements basic S3 commands.  Now I want to create a higher level function, that mimics the features of both PyTorch data loader and TensorFlow data loader functions.

First, analyze sd3lio repo here: https://github.com/russfellows/s3dlio/tree/main

Now, analyze TensorFlow's data, and data loader features: 
https://www.tensorflow.org/api_docs/python/tf/data
https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset

Also examine the options to add threading: https://www.tensorflow.org/api_docs/python/tf/data/ThreadingOptions

Along with randomization, and other auto-tuning features for performance: https://www.tensorflow.org/api_docs/python/tf/data/experimental

Finally, let's examine PyTorch's data loader capabilities: https://docs.pytorch.org/docs/stable/data.html

With all of this in mind, let's create a data loader function for s3dlio, that has similar capabilities, including batching, map and iterable datasets, randomization, etc.  

Please map out a plan for implementing these changes to sd3lio, including what files to add and update.  I would like a plan that implements this in 2 or 3 steps to ensure full functionality, both in the core s3dlio library, and in the Python library.


## Chat Reply
Below is a concrete, three‑stage implementation plan that upgrades s3dlio from a collection of low‑level S3 helpers into a fully‑featured, framework‑agnostic data‑loading stack comparable to tf.data and PyTorch DataLoader. Each stage cleanly compiles, ships tests, and can be published independently, giving you clear checkpoints and minimizing regressions.

## Stage 1 – Core abstractions

| Goal                                                                                                                                  | Deliverables                                                                                                                                                                                                                                                                                                                                         | Files to add / modify                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Introduce a stable, idiomatic Rust API for reading objects in batches (sequential pipeline). Expose the same functionality to Python. | \* **Trait hierarchy**<br>  • `Dataset`: random or sequential access to *samples* (objects or records).<br>  • `DataLoader`: high‑level iterator that yields *batches*.<br>\* First‑class dataset implementations for the four object types already supported by s3dlio (RAW, NPZ, HDF5, TFRecord).<br>\* Round‑trip smoke tests in Rust and Python. | **Rust** (src/)  <br>• `dataset.rs` – defines `Dataset` trait, common helpers. <br>• `dataloader.rs` – defines `DataLoader`, simple sequential iterator. <br>• `formats/{raw,npz,hdf5,tfrecord}.rs` – implement `Dataset` for each type (may wrap existing helpers). <br>• `options.rs` – `LoaderOptions { batch_size, drop_last }`.<br>• **mod** declarations in `lib.rs` to re‑export.<br><br>**Python** (python/s3dlio/)  <br>• `dataset.py` – thin `PyClass` wrappers generated by `pyo3`. <br>• `dataloader.py` – PEP 484‑typed façade that mirrors the Rust API and implements `__iter__`/`__next__`. <br><br>**Tests**  <br>• `tests/test_dataloader.rs` – read 10 randomly generated NPZ objects from MinIO bucket.<br>• `tests/test_python_loader.py` – equivalent pytest. |

### Key design decisions

* Keep the Rust side pure‑async with Tokio; the iterator uses an internal stream::unfold so later we can plug in parallelism without changing the public API.
* Datasets expose length (len() -> Option<usize>) so the loader can automatically compute __len__ in Python for map‑style datasets, mirroring torch.utils.data semantics 
* PyTorch Documentation
* All sample decoding stays in Rust (HDF5 via hdf5‑rs, NPZ via ndarray‑npy). Python only gets zero‑copy memoryviews.

## Stage 2 – Performance & convenience features
| Goal                                                                                                      | Deliverables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Files / changes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Add the heavy‑hitter functionality users expect: shuffling, parallel fetch/decode, prefetch, auto‑tuning. | \* `ShufflingSampler` and `SequentialSampler` (implements **PyTorch‑style** `Sampler` interface).<br>\* `LoaderOptions` gains:<br>  • `shuffle: bool`, `seed: u64` – Fisher‑Yates in Rust for determinism.<br>  • `num_workers: usize` – rayon thread‑pool for local CPU decoding; Tokio tasks for S3 traffic.<br>  • `prefetch: usize` – bounded channel that overlaps I/O/compute.<br>  • `auto_tune: bool` – if true, start small and double `prefetch` / workers until producer keeps pace (simple heuristic similar to **`tf.data.AUTOTUNE`** ([TensorFlow][1])).<br>\* `map`, `filter`, `flat_map` combinators on `Dataset` (returning new dataset wrappers).<br>\* `dataloader.rs` now selects between sequential and shuffled samplers, spins up worker pool, and supports `.with_options()` builder. | **Rust**  <br>• `sampler.rs`, `transform.rs`, `prefetch.rs`. <br>• Extend `Cargo.toml` with `rayon = {version="*", features=["threads"]}`.<br><br>**Python**  <br>• `threading.py` – expose a thin wrapper that mimics **`tf.data.ThreadingOptions`** (max intra‑/inter‑op) ([TensorFlow][2]).<br>\* Loader gains `.map`, `.batch`, `.prefetch`, `.shuffle`, `.options(...)` so one can write one‑liners like in TF:<br>`python\n(ds.shuffle(10000)\n   .map(parse_fn, num_parallel_calls='auto')\n   .batch(128, drop_last=True)\n   .prefetch('auto'))` |

[1]: https://www.tensorflow.org/guide/data_performance?utm_source=chatgpt.com "Better performance with the tf.data API - TensorFlow Core"
[2]: https://www.tensorflow.org/api_docs/python/tf/data/ThreadingOptions?utm_source=chatgpt.com "tf.data.ThreadingOptions - TensorFlow v2.16.1"


### Concurrency model

* S3 GET/PUT/List remains async; workers are lightweight Tokio tasks limited by jobs (existing option) so we avoid over‑subscribing sockets.
* Decoding and batched collation uses rayon when CPU‑bound; you can pin a per‑loader thread‑pool size or fall back to num_cpus::get().

## Stage 3 – Framework adapters, examples & docs
| Goal                                                                                                                                                            | Deliverables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Files / changes                                                                                                                                                                                                                                                                                                                             |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Provide drop‑in adapters so users can feed s3dlio directly to **PyTorch**, **TensorFlow/Keras**, and plain NumPy workflows.  Finalize docs and release on PyPI. | \* **PyTorch**: `python/s3dlio/torch.py` containing<br>  • `class TorchIterableDataset(torch.utils.data.IterableDataset)` that wraps an `s3dlio.DataLoader`.<br>  • `class TorchMapDataset(torch.utils.data.Dataset)` when the underlying dataset reports length.<br>\* **TensorFlow**: `tf_dataset = tf.data.Dataset.from_generator(loader.generator_signature, output_signature, args=[loader])` helper in `python/s3dlio/tf.py` (mirrors TFRecordDataset signatures) ([TensorFlow][1]).<br>\* CLI example: `python/examples/train_torch.py`, `train_tf.py`.<br>\* ReStructuredText / MkDocs page under **docs/** that cross‑links to the TensorFlow *Better performance with tf.data* guide ([TensorFlow][2]) and explains tuning knobs.<br>\* GitHub CI job that spins up a local MinIO container, uploads 100 mock NPZ objects, and runs the Torch and TF unit tests. | **Python**  <br>• new submodules `torch.py`, `tf.py`, `examples/`. <br><br>**Rust**  <br>• `lib.rs` exports `pub mod adapter` only for Python compilation feature (behind `#[cfg(feature="python")]`). <br><br>**Doc & Release**  <br>• `docs/data_loader.md`, update `README.md` usage section. <br>• Tag v0.3.0 and upload wheel to PyPI. |

[1]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset?utm_source=chatgpt.com "tf.data.Dataset | TensorFlow v2.16.1"
[2]: https://www.tensorflow.org/guide/data_performance?utm_source=chatgpt.com "Better performance with the tf.data API - TensorFlow Core"



## How this lines up with existing s3dlio code‑base

The new modules live beside the current low‑level S3 ops (src/s3/…). No breaking change to get/put/list/stat/delete—the loader is an optional higher layer, exactly as promised in the project overview 
GitHub
.
The CLI remains useful for performance benchmarking; you may add a dataload sub‑command later, but that is outside the critical path.
Python wheels still build with Maturin; just add features = ["extension-module", "loader"] so pure‑Rust users can opt‑out and keep a lean binary.
Suggested timeline
Week	Milestone
1	Stage 1 PR merged, nightly wheel published to internal PyPI.
2‑3	Stage 2 perf features; profile with 4 × GPU loader to confirm no regressions; cut v0.2.0 release.
4	Stage 3 adapters, docs, CI; tag v0.3.0 & announce.
Final notes & tips
Testing at scale — for shuffle correctness, seed a 10 k‑object bucket, iterate twice with the same seed and assert identical order; change seed and assert difference. Mirrors PyTorch behaviour 
PyTorch Forums
.
Memory footprint — for multi‑worker decode, allocate a single page‑aligned arena per thread and reuse buffers (aligns with your earlier efficiency goal).
Future work — consider adding parquet and arrow IPC Dataset back‑ends once the abstraction stabilizes.
With these three incremental steps, s3dlio will offer a first‑class, high‑performance data‑loading experience while staying true to its S3‑native, Rust‑powered roots.

# Final notes & tips
* Testing at scale — for shuffle correctness, seed a 10 k‑object bucket, iterate twice with the same seed and assert identical order; change seed and assert difference. Mirrors PyTorch behaviour 
* Memory footprint — for multi‑worker decode, allocate a single page‑aligned arena per thread and reuse buffers (aligns with your earlier efficiency goal).
* Future work — consider adding parquet and arrow IPC Dataset back‑ends once the abstraction stabilizes.
