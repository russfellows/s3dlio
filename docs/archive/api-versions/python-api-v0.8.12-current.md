# S3DLIO Python API Reference - Version 0.8.12 (Current)

**‚ö†Ô∏è IMPORTANT VERSION INFORMATION**
- **This document**: For s3dlio v0.8.12+ (October 2025)
- **Previous API**: See `python_api_legacy_v0.7.md` for older versions
- **Status**: This is the CURRENT and RECOMMENDED API

---

## üÜï **NEW Enhanced API (v0.8.0+)** - **‚úÖ WORKING**

The enhanced API provides a unified interface across all storage backends with generic factory functions.

### Core Functions (‚úÖ WORKING)

#### `create_dataset(uri: str, options: dict = None) -> PyDataset`
**Status**: ‚úÖ **FULLY WORKING** - Main dataset creation function

```python
import s3dlio

# ‚úÖ WORKING: File system dataset
dataset = s3dlio.create_dataset("file:///path/to/data")

# ‚úÖ WORKING: S3 dataset (requires AWS credentials)
dataset = s3dlio.create_dataset("s3://bucket/prefix/")

# ‚úÖ WORKING: With options
dataset = s3dlio.create_dataset("file:///data", {
    "batch_size": 32,
    "shuffle": True
})
```

#### `create_async_loader(uri: str, options: dict = None) -> PyBytesAsyncDataLoader`
**Status**: ‚úÖ **FULLY WORKING** - Async data loading

```python
import s3dlio
import asyncio

# ‚úÖ WORKING: Create async loader
loader = s3dlio.create_async_loader("file:///path/to/data")

# ‚úÖ WORKING: Async iteration
async def process():
    async for item in loader:
        print(f"Got {len(item)} bytes")

asyncio.run(process())
```

### Core Classes (‚úÖ WORKING)

#### `PyDataset` 
**Status**: ‚úÖ **AVAILABLE** - Generic dataset container

```python
# ‚úÖ WORKING: Dataset creation returns PyDataset
dataset = s3dlio.create_dataset("file:///data")
print(type(dataset))  # <class 'builtins.PyDataset'>
```

#### `PyBytesAsyncDataLoader`
**Status**: ‚úÖ **AVAILABLE** - Async data loader

```python
# ‚úÖ WORKING: Async loader creation returns PyBytesAsyncDataLoader
loader = s3dlio.create_async_loader("file:///data")
print(type(loader))  # <class 'builtins.PyBytesAsyncDataLoader'>
```

### URI Schemes (‚úÖ WORKING)

#### File System (`file://`) - ‚úÖ **FULLY WORKING**
```python
# ‚úÖ WORKING: Single file
s3dlio.create_dataset("file:///home/user/data.txt")

# ‚úÖ WORKING: Directory (recursive scan)
s3dlio.create_dataset("file:///home/user/dataset/")

# ‚úÖ WORKING: Network paths
s3dlio.create_dataset("file:///mnt/vast1/data/")
```

#### Amazon S3 (`s3://`) - ‚úÖ **WORKING** (requires credentials)
```python
# ‚úÖ WORKING: S3 prefix
s3dlio.create_dataset("s3://bucket/prefix/")

# ‚úÖ WORKING: Single S3 object
s3dlio.create_dataset("s3://bucket/object.bin")
```

#### Azure (`az://`) - üöß **FRAMEWORK READY**
```python
# üöß FRAMEWORK EXISTS: Will be completed in future release
s3dlio.create_dataset("az://container/prefix/")
```

#### Direct I/O (`direct://`) - üöß **FRAMEWORK READY**  
```python
# üöß FRAMEWORK EXISTS: Will be completed in future release
s3dlio.create_dataset("direct:///dev/nvme0n1")
```

---

## üîß **Legacy API (v0.7.x and earlier)** - **‚ö†Ô∏è MIXED STATUS**

These functions are preserved for backward compatibility but have varying status.

### Core Functions - **Status Varies**

#### `get(uri: str) -> bytes` - ‚úÖ **WORKING**
```python
# ‚úÖ WORKING: Get single object
data = s3dlio.get("s3://bucket/object.txt")
```

#### `put(uri: str, data: bytes)` - ‚úÖ **WORKING**
```python
# ‚úÖ WORKING: Put single object
s3dlio.put("s3://bucket/object.txt", b"data")
```

#### `put(prefix, num, template, ...)` - ‚úÖ **WORKING** (v0.8.2+)
**NEW in v0.8.2**: Configurable data generation modes for optimal performance

```python
# ‚úÖ WORKING: Bulk object creation with default streaming mode
s3dlio.put(
    prefix="s3://bucket/object-{}.bin",
    num=10,
    template="obj-{}-of-{}",
    size=4194304,  # 4MB objects
    max_in_flight=64
)

# ‚úÖ WORKING: Explicit data generation mode selection (v0.8.2+)
s3dlio.put(
    prefix="s3://bucket/object-{}.bin", 
    num=10,
    template="obj-{}-of-{}",
    size=4194304,
    data_gen_mode="streaming",  # Default: optimal for most cases
    chunk_size=262144          # Default chunk size
)

# ‚úÖ WORKING: Single-pass mode for specific use cases
s3dlio.put(
    prefix="s3://bucket/object-{}.bin",
    num=10, 
    template="obj-{}-of-{}",
    size=16777216,             # 16MB objects
    data_gen_mode="single-pass", # Alternative mode
    chunk_size=65536
)
```

**Parameters (v0.8.2+)**:
- `prefix`: S3 URI template with `{}` placeholder for object names
- `num`: Number of objects to create
- `template`: Template for object naming (use `{}` for index and total)
- `size`: Size of each object in bytes
- `max_in_flight`: Maximum concurrent uploads (default: 64)
- `data_gen_mode`: **NEW** - `"streaming"` (default) or `"single-pass"`  
- `chunk_size`: **NEW** - Chunk size for data generation (default: 262144)
- `object_type`: Object content type (default: `"zeros"`)
- `dedup_factor`: Deduplication factor (default: 1)
- `compress_factor`: Compression factor (default: 1)

**Performance Notes (v0.8.2+)**:
- **Streaming mode** (default): 2.6-3.5x faster for 1-8MB objects, wins in 64% of scenarios
- **Single-pass mode**: May be competitive for 16-32MB objects in specific cases
- **Automatic optimization**: Streaming is set as default based on comprehensive benchmarking

#### `list(uri: str) -> List[str]` - ‚úÖ **WORKING**
```python  
# ‚úÖ WORKING: List objects
keys = s3dlio.list("s3://bucket/prefix/")
```

#### `stat(uri: str) -> dict` - ‚úÖ **WORKING**
```python
# ‚úÖ WORKING: Get object metadata
metadata = s3dlio.stat("s3://bucket/object.txt")
```

### PyTorch Integration

#### `S3IterableDataset` - **üêõ FIXED in v0.8.0**
```python
from s3dlio.torch import S3IterableDataset

# ‚úÖ NOW WORKING: Bug fixed in v0.8.0
# Previously failed with "PyS3AsyncDataLoader not found"
dataset = S3IterableDataset("file:///data/")

# ‚úÖ WORKING: PyTorch DataLoader integration
from torch.utils.data import DataLoader
dataloader = DataLoader(dataset, batch_size=32)
```

**Bug History**:
- **v0.7.x and earlier**: ‚ùå Failed with "PyS3AsyncDataLoader not found" 
- **v0.8.0+**: ‚úÖ Fixed - now uses `create_async_loader` internally

### Legacy Classes - **‚ö†Ô∏è DEPRECATED or PROBLEMATIC**

#### `PyS3Dataset` - ‚ö†Ô∏è **DEPRECATED**
```python
# ‚ö†Ô∏è DEPRECATED: Use create_dataset() instead
# May still work but not recommended
if hasattr(s3dlio, 'PyS3Dataset'):
    old_dataset = s3dlio.PyS3Dataset("s3://bucket/data/")
```

#### `PyS3AsyncDataLoader` - ‚ùå **WAS BROKEN, NOW FIXED**
```python
# ‚ùå v0.7.x: This class didn't exist, causing PyTorch errors
# ‚úÖ v0.8.0+: Replaced with generic create_async_loader()

# üö´ DON'T USE: This was the source of the original bug
# ‚úÖ USE INSTEAD: s3dlio.create_async_loader()
```

---

## üß™ **What Actually Works - Test Status**

### ‚úÖ **Verified Working Functions**

Based on actual testing, these functions are confirmed working:

```python
import s3dlio

# ‚úÖ CONFIRMED: These functions exist and work
create_dataset = getattr(s3dlio, 'create_dataset')        # ‚úÖ Works
create_async_loader = getattr(s3dlio, 'create_async_loader')  # ‚úÖ Works

# ‚úÖ CONFIRMED: Legacy functions work  
get_func = getattr(s3dlio, 'get')                        # ‚úÖ Works
put_func = getattr(s3dlio, 'put')                        # ‚úÖ Works
list_func = getattr(s3dlio, 'list')                      # ‚úÖ Works
stat_func = getattr(s3dlio, 'stat')                      # ‚úÖ Works
```

### ‚ö†Ô∏è **Known Issues**

#### Linter vs Runtime Discrepancy
```python
# ‚ö†Ô∏è LINTER ISSUE: Shows as "not found" but actually works
# This is a linter/IDE problem, not a runtime problem
dataset = s3dlio.create_dataset("file:///data")  # ‚úÖ Actually works!
```

#### PyDataset Iteration  
```python
# üöß NEEDS INVESTIGATION: Direct iteration may have issues
dataset = s3dlio.create_dataset("file:///data")
# This may not work: for item in dataset
# But the dataset object is created successfully
```

#### S3IterableDataset Parameters
```python  
# ‚ö†Ô∏è PARAMETER ISSUE: May require loader_opts parameter
from s3dlio.torch import S3IterableDataset
# This might need: S3IterableDataset("uri", loader_opts={})
```

---

## üìã **Migration Checklist**

### From v0.7.x to v0.8.0

#### ‚úÖ **No Changes Needed** (Backward Compatible)
- All existing `get()`, `put()`, `list()`, `stat()` calls work unchanged
- Existing S3IterableDataset usage now works (bug was fixed internally)

#### üÜï **New Capabilities Available**
- Use `create_dataset()` for unified dataset creation
- Use `create_async_loader()` for async data processing  
- File system support with `file://` URIs
- Multi-backend support ready for expansion

#### ‚ö†Ô∏è **Recommended Updates**
```python
# OLD (still works)
data = s3dlio.get("s3://bucket/object")

# NEW (recommended for consistency)
dataset = s3dlio.create_dataset("s3://bucket/object")

# OLD PyTorch usage (now works, was broken)
from s3dlio.torch import S3IterableDataset
dataset = S3IterableDataset("s3://bucket/data/")  # Now works!

# NEW PyTorch usage (also works)
loader = s3dlio.create_async_loader("s3://bucket/data/")
```

---

## üîç **Error Handling**

### Working Error Detection
```python
# ‚úÖ WORKING: Error handling is functional
try:
    dataset = s3dlio.create_dataset("ftp://invalid/scheme")
except RuntimeError as e:
    print(f"Expected error: {e}")  # Properly caught

try:
    dataset = s3dlio.create_dataset("file:///nonexistent/path")  
except RuntimeError as e:
    print(f"Path error: {e}")  # Properly caught
```

### Common Error Types
- `RuntimeError`: Invalid URIs, missing paths, backend issues
- `Exception`: General configuration or parameter problems

---

## ÔøΩ **Operation Logging (Op-Log)** - **üÜï v0.8.11**

The op-log feature enables performance analysis and debugging by tracing all storage operations to a compressed TSV file. Supports all backends: `file://`, `s3://`, `az://`, and `direct://`.

### Quick Start

```python
import s3dlio

# 1. Initialize op-log at the start
s3dlio.init_op_log("/tmp/my_operations.tsv.zst")

# 2. Check if active (optional)
if s3dlio.is_op_log_active():
    print("Op-log is recording")

# 3. Perform your operations - ALL are automatically logged
s3dlio.upload(
    src_patterns=["./data/*.dat"],
    dest_prefix="file:///tmp/output/",
    max_in_flight=4,
    create_bucket=False
)

s3dlio.download(
    src_uri="file:///tmp/output/",
    dest_dir="./downloads/",
    max_in_flight=4,
    recursive=True
)

# 4. Finalize op-log at the end
s3dlio.finalize_op_log()
```

### Op-Log Functions

#### `init_op_log(path: str)`
Initialize operation logging to a compressed TSV file.

**Parameters:**
- `path`: Output file path (will be created with `.zst` extension)

**Example:**
```python
s3dlio.init_op_log("/tmp/performance_trace.tsv.zst")
```

#### `is_op_log_active() -> bool`
Check if operation logging is currently active.

**Returns:** `True` if op-log is recording, `False` otherwise

**Example:**
```python
if s3dlio.is_op_log_active():
    print("Operations are being traced")
```

#### `finalize_op_log()`
Finalize and flush the operation log file.

**Example:**
```python
s3dlio.finalize_op_log()
print("Op-log saved successfully")
```

### Op-Log Format

The op-log file is a zstd-compressed TSV with the following columns:

```
idx  thread  op  client_id  n_objects  bytes  endpoint  file  error  start  first_byte  end  duration_ns
```

**Columns:**
- `idx`: Sequential operation index
- `thread`: Thread identifier (hash)
- `op`: Operation type (`PUT`, `GET`, `LIST`, etc.)
- `client_id`: Client/session identifier
- `n_objects`: Number of objects involved
- `bytes`: Bytes transferred
- `endpoint`: Storage endpoint (e.g., `file://`, `s3://bucket`)
- `file`: File path (without URI scheme prefix)
- `error`: Error message if operation failed
- `start`: Operation start timestamp (ISO 8601)
- `first_byte`: Time of first byte (ISO 8601)
- `end`: Operation end timestamp (ISO 8601)
- `duration_ns`: Duration in nanoseconds

### Complete Example

```python
import s3dlio
import os

def process_data_with_logging():
    """Process data with full operation tracing."""
    
    # Initialize op-log
    log_file = "/tmp/data_processing.tsv.zst"
    s3dlio.init_op_log(log_file)
    print(f"Op-log initialized: {log_file}")
    
    try:
        # Upload files to file:// backend
        s3dlio.upload(
            src_patterns=["./input/*.dat"],
            dest_prefix="file:///tmp/staging/",
            max_in_flight=8,
            create_bucket=False
        )
        
        # Download from file:// backend
        s3dlio.download(
            src_uri="file:///tmp/staging/",
            dest_dir="/tmp/processed/",
            max_in_flight=8,
            recursive=True
        )
        
        # Upload to DirectIO storage
        s3dlio.upload(
            src_patterns=["/tmp/processed/*.dat"],
            dest_prefix="direct:///fast/storage/",
            max_in_flight=16,
            create_bucket=False
        )
        
        print("All operations completed successfully")
        
    finally:
        # Always finalize the log
        s3dlio.finalize_op_log()
        
        # Verify log file
        if os.path.exists(log_file):
            size = os.path.getsize(log_file)
            print(f"Op-log saved: {log_file} ({size} bytes)")

if __name__ == "__main__":
    process_data_with_logging()
```

### Notes

- **Zero overhead**: When op-log is not initialized, there's no performance impact
- **Thread-safe**: Safe to use with concurrent operations
- **All backends**: Works with file://, s3://, az://, and direct:// URIs
- **Automatic**: Once initialized, all operations are logged transparently
- **Compatible**: Log format is compatible with warp-replay tool

---

## ÔøΩüìö **Additional Documentation**

- **Enhanced API Reference**: `enhanced-api-v0.8.0.md` - Complete technical details
- **Migration Guide**: `migration-guide-v0.8.0.md` - Detailed migration instructions  
- **Legacy API Reference**: `python_api_legacy_v0.7.md` - For older versions
- **Examples**: `examples/enhanced_api_examples.py` - Working code examples

---

## üè∑Ô∏è **Version History**

- **v0.8.0** (September 2025): Enhanced API, bug fixes, multi-backend support
- **v0.7.x** (Earlier): Original API, PyTorch bug present  
- **v0.6.x and earlier**: See legacy documentation

---

**üí° RECOMMENDATION**: Use the new enhanced API (`create_dataset`, `create_async_loader`) for new projects while existing code continues to work unchanged.