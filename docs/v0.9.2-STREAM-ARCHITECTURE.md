# v0.9.2 Stream-Based Architecture Implementation

## Overview
This release refactors s3dlio to use **streams as the universal substrate** for all async I/O operations:
1. Generic RangeEngine decoupled from S3
2. Stream-based concurrent downloads for all backends
3. Unified backpressure, cancellation, and ordering control
4. Consistent configuration surface across all components

## Current Architecture Analysis

### What's Already Working ‚úÖ

**1. DataLoader Streaming (src/data_loader/dataloader.rs)**
```rust
pub fn stream(self) -> ReceiverStream<Result<Vec<D::Item>, DatasetError>> {
    // Bounded MPSC channel for backpressure
    let (tx, rx) = mpsc::channel::<...>(self.options.prefetch);
    // Returns ReceiverStream - already a clean Stream impl
    ReceiverStream::new(rx)
}
```
- ‚úÖ Uses `ReceiverStream` over bounded MPSC
- ‚úÖ Prefetch = channel capacity (backpressure)
- ‚úÖ Clean `Stream<Item = Result<Vec<Item>>>` interface

**2. AsyncPoolDataLoader (src/data_loader/async_pool_dataloader.rs)**
```rust
pub fn stream_with_pool(self, pool_config: PoolConfig) -> ReceiverStream<...> {
    // Pool of concurrent requests with FuturesUnordered
    let mut pending_requests: FuturesUnordered<RequestFuture> = ...;
    // Maintains pool_size concurrent requests
    // Forms batches out-of-order
    // Bounded channel for backpressure
}
```
- ‚úÖ Uses `FuturesUnordered` for concurrent requests
- ‚úÖ Maintains steady pool size
- ‚úÖ Out-of-order batch formation
- ‚úÖ Timeout per request
- ‚ö†Ô∏è NO cancellation token (keeps running after consumer drops)
- ‚ö†Ô∏è Doesn't implement `Stream` trait directly (returns `ReceiverStream`)

**3. S3 Range Reader (src/data_loader/s3_bytes.rs)**
```rust
let mut chunks = stream::iter(0..n_parts)
    .map(|i| async move {
        get_object_range_uri_async(&uri, start, Some(len)).await
    })
    .buffered(max_inflight);
```
- ‚úÖ Perfect use of `stream::iter().buffered()` pattern
- ‚úÖ Concurrent range fetches with controlled parallelism
- ‚úÖ Ordered assembly of results
- ‚ö†Ô∏è S3-specific (not available for other backends)

**4. GCS Batch Operations (src/gcs_client.rs)**
```rust
let results: Vec<Result<()>> = stream::iter(objects)
    .map(|object| async move { ... })
    .buffered(max_concurrent)
    .collect()
    .await;
```
- ‚úÖ Uses stream combinators for concurrency
- ‚úÖ Clean pattern for batch operations

### What Needs Improvement ‚ùå

**1. RangeEngine (src/range_engine.rs)**
```rust
pub struct RangeEngineConfig {
    pub clients: Arc<ShardedS3Clients>,  // ‚ùå Hardcoded to S3!
    // ...
}
```
- ‚ùå Tightly coupled to `ShardedS3Clients`
- ‚ùå Cannot be used by other backends
- ‚ùå Only tested, never integrated into production paths

**2. No Cancellation Infrastructure**
- ‚ùå Pool workers keep running after consumer drops
- ‚ùå No clean shutdown mechanism
- ‚ùå Prefetch loops don't stop when stream ends

**3. Configuration Inconsistency**
Multiple overlapping knobs:
- `LoaderOptions`: `prefetch`, `batch_size`, `reader_mode`, `part_size`, `max_inflight_parts`
- `PoolConfig`: `pool_size`, `readahead_batches`, `batch_timeout`
- `RangeEngineConfig`: `range_size`, `max_concurrent_ranges`, `min_split_size`

**4. No Ordering Control**
- `FuturesUnordered` only (out-of-order)
- No option for `FuturesOrdered` (in-order completion)

**5. ObjectStore `get()` Not Stream-Based**
All backends use simple sequential downloads:
```rust
async fn get(&self, uri: &str) -> Result<Bytes> {
    // Simple single request, no concurrency
}
```

---

## Implementation Design

### Phase 1: Generic RangeEngine with Streams

#### 1.1 New RangeEngine Design

**File**: `src/range_engine_generic.rs` (new)

```rust
use bytes::Bytes;
use futures::stream::{self, StreamExt};
use tokio::sync::Semaphore;
use tokio_util::sync::CancellationToken;
use anyhow::Result;
use std::sync::Arc;

/// Configuration for generic range-based downloads
#[derive(Debug, Clone)]
pub struct RangeEngineConfig {
    /// Size of each range chunk (default: 64MB)
    pub chunk_size: usize,
    
    /// Maximum concurrent range requests (default: 32)
    pub max_concurrent_ranges: usize,
    
    /// Minimum object size to trigger range splitting (default: 4MB)
    pub min_split_size: u64,
    
    /// Timeout per range request (default: 30s)
    pub range_timeout: Duration,
}

impl Default for RangeEngineConfig {
    fn default() -> Self {
        Self {
            chunk_size: 64 * 1024 * 1024,      // 64MB
            max_concurrent_ranges: 32,
            min_split_size: 4 * 1024 * 1024,   // 4MB
            range_timeout: Duration::from_secs(30),
        }
    }
}

/// Statistics for range download operation
#[derive(Debug, Clone)]
pub struct RangeDownloadStats {
    pub bytes_downloaded: u64,
    pub ranges_processed: usize,
    pub elapsed_time: Duration,
    pub throughput_bps: u64,
}

/// Generic range-based download engine
/// 
/// Works with ANY backend that implements `get_range(offset, length)`
pub struct RangeEngine {
    config: RangeEngineConfig,
    concurrency_limiter: Arc<Semaphore>,
}

impl RangeEngine {
    pub fn new(config: RangeEngineConfig) -> Self {
        let concurrency_limiter = Arc::new(Semaphore::new(config.max_concurrent_ranges));
        Self { config, concurrency_limiter }
    }

    /// Download object using concurrent range requests
    /// 
    /// Generic over any async function that can fetch a range.
    /// Uses stream-based architecture with controlled concurrency.
    pub async fn download<F, Fut>(
        &self,
        object_size: u64,
        get_range: F,
        cancel: Option<CancellationToken>,
    ) -> Result<(Bytes, RangeDownloadStats)>
    where
        F: Fn(u64, u64) -> Fut + Send + Sync + Clone + 'static,
        Fut: std::future::Future<Output = Result<Bytes>> + Send,
    {
        let start_time = std::time::Instant::now();
        
        // Small objects: single request
        if object_size < self.config.min_split_size {
            let bytes = get_range(0, object_size).await?;
            let stats = RangeDownloadStats {
                bytes_downloaded: bytes.len() as u64,
                ranges_processed: 1,
                elapsed_time: start_time.elapsed(),
                throughput_bps: self.calculate_throughput(bytes.len() as u64, start_time),
            };
            return Ok((bytes, stats));
        }
        
        // Large objects: concurrent ranges with streams
        self.download_with_ranges(object_size, get_range, cancel, start_time).await
    }

    /// Download using concurrent range requests (stream-based)
    async fn download_with_ranges<F, Fut>(
        &self,
        object_size: u64,
        get_range: F,
        cancel: Option<CancellationToken>,
        start_time: std::time::Instant,
    ) -> Result<(Bytes, RangeDownloadStats)>
    where
        F: Fn(u64, u64) -> Fut + Send + Sync + Clone + 'static,
        Fut: std::future::Future<Output = Result<Bytes>> + Send,
    {
        // Calculate ranges
        let ranges = self.calculate_ranges(object_size);
        let n_ranges = ranges.len();
        
        // Create stream of range requests
        let semaphore = Arc::clone(&self.concurrency_limiter);
        let timeout = self.config.range_timeout;
        
        let mut chunks = stream::iter(ranges)
            .enumerate()
            .map(|(idx, (offset, length))| {
                let get_range = get_range.clone();
                let semaphore = Arc::clone(&semaphore);
                let cancel = cancel.clone();
                
                async move {
                    // Check cancellation
                    if let Some(ref token) = cancel {
                        if token.is_cancelled() {
                            return Err(anyhow::anyhow!("Download cancelled"));
                        }
                    }
                    
                    // Acquire concurrency permit
                    let _permit = semaphore.acquire().await
                        .map_err(|e| anyhow::anyhow!("Semaphore error: {}", e))?;
                    
                    // Execute range request with timeout
                    let bytes = tokio::time::timeout(timeout, get_range(offset, length))
                        .await
                        .map_err(|_| anyhow::anyhow!("Range request timeout"))?
                        .map_err(|e| anyhow::anyhow!("Range request failed: {}", e))?;
                    
                    Ok((idx, bytes))
                }
            })
            .buffered(self.config.max_concurrent_ranges);
        
        // Collect and reassemble in order
        let mut parts: Vec<(usize, Bytes)> = Vec::with_capacity(n_ranges);
        while let Some(result) = chunks.next().await {
            let (idx, bytes) = result?;
            parts.push((idx, bytes));
        }
        
        // Sort by index to maintain order
        parts.sort_by_key(|(idx, _)| *idx);
        
        // Assemble final buffer
        let total_size: usize = parts.iter().map(|(_, b)| b.len()).sum();
        let mut assembled = Vec::with_capacity(total_size);
        for (_, bytes) in parts {
            assembled.extend_from_slice(&bytes);
        }
        
        let bytes_downloaded = assembled.len() as u64;
        let stats = RangeDownloadStats {
            bytes_downloaded,
            ranges_processed: n_ranges,
            elapsed_time: start_time.elapsed(),
            throughput_bps: self.calculate_throughput(bytes_downloaded, start_time),
        };
        
        Ok((Bytes::from(assembled), stats))
    }

    /// Calculate optimal range splits
    fn calculate_ranges(&self, object_size: u64) -> Vec<(u64, u64)> {
        let mut ranges = Vec::new();
        let mut offset = 0u64;
        let chunk_size = self.config.chunk_size as u64;
        
        while offset < object_size {
            let remaining = object_size - offset;
            let length = remaining.min(chunk_size);
            ranges.push((offset, length));
            offset += length;
        }
        
        ranges
    }

    fn calculate_throughput(&self, bytes: u64, start: std::time::Instant) -> u64 {
        let elapsed = start.elapsed().as_secs_f64();
        if elapsed > 0.0 {
            (bytes as f64 / elapsed) as u64
        } else {
            0
        }
    }
}
```

**Key Features**:
- ‚úÖ Generic over any `get_range()` function (closure-based)
- ‚úÖ Stream-based with `stream::iter().buffered()`
- ‚úÖ Cancellation token support
- ‚úÖ Timeout per range request
- ‚úÖ Ordered assembly of results
- ‚úÖ Backpressure via semaphore
- ‚úÖ Zero S3-specific code

#### 1.2 Integration Pattern for ObjectStore

**Each backend modifies its `get()` method**:

```rust
// src/file_store.rs
impl ObjectStore for FileSystemObjectStore {
    async fn get(&self, uri: &str) -> Result<Bytes> {
        if !uri.starts_with("file://") { 
            bail!("FileSystemObjectStore expected file:// URI"); 
        }
        
        let path = Self::uri_to_path(uri)?;
        
        // Check if file exists
        if !path.exists() || !path.is_file() {
            bail!("File not found: {}", path.display());
        }
        
        // Get file size
        let metadata = fs::metadata(&path).await?;
        let file_size = metadata.len();
        
        // Decide: use range engine or simple read?
        if file_size >= self.config.range_engine_threshold {
            // Use generic RangeEngine for large files
            self.get_with_range_engine(uri, file_size).await
        } else {
            // Simple read for small files
            self.get_simple(uri).await
        }
    }
    
    async fn get_with_range_engine(&self, uri: &str, size: u64) -> Result<Bytes> {
        let engine = RangeEngine::new(self.config.range_engine_config.clone());
        
        // Create closure that captures 'self' and 'uri'
        let uri_owned = uri.to_string();
        let self_clone = self.clone(); // Requires Clone on ObjectStore or Arc wrapper
        
        let get_range_fn = move |offset: u64, length: u64| {
            let uri = uri_owned.clone();
            let store = self_clone.clone();
            async move {
                store.get_range(&uri, offset, Some(length)).await
            }
        };
        
        let (bytes, stats) = engine.download(size, get_range_fn, None).await?;
        
        tracing::debug!(
            "RangeEngine: downloaded {} bytes in {} ranges, throughput: {} MB/s",
            stats.bytes_downloaded,
            stats.ranges_processed,
            stats.throughput_bps / 1_000_000
        );
        
        Ok(bytes)
    }
    
    async fn get_simple(&self, uri: &str) -> Result<Bytes> {
        let path = Self::uri_to_path(uri)?;
        let file = fs::File::open(&path).await?;
        let std_file = file.try_into_std()
            .map_err(|_| anyhow::anyhow!("Failed to convert to std file"))?;
        
        let metadata = fs::metadata(&path).await?;
        let _ = apply_page_cache_hint(&std_file, PageCacheMode::Auto, metadata.len());
        
        let mut file = fs::File::from_std(std_file);
        let mut data = Vec::new();
        file.read_to_end(&mut data).await?;
        
        Ok(Bytes::from(data))
    }
}
```

**Similar pattern for**:
- DirectIO backend
- Azure backend
- GCS backend
- S3 backend (can use generic RangeEngine instead of sharded_client logic)

---

### Phase 2: Cancellation Infrastructure

#### 2.1 Add CancellationToken to DataLoader

**File**: `src/data_loader/async_pool_dataloader.rs`

```rust
use tokio_util::sync::CancellationToken;

pub struct AsyncPoolDataLoader {
    dataset: Arc<MultiBackendDataset>,
    options: LoaderOptions,
    cancel_token: CancellationToken,  // NEW
}

impl AsyncPoolDataLoader {
    pub fn new(dataset: MultiBackendDataset, options: LoaderOptions) -> Self {
        Self {
            dataset: Arc::new(dataset),
            options,
            cancel_token: CancellationToken::new(),  // NEW
        }
    }
    
    pub fn stream_with_pool(self, pool_config: PoolConfig) -> ReceiverStream<...> {
        let (tx, rx) = mpsc::channel::<...>(pool_config.readahead_batches);
        
        let cancel = self.cancel_token.clone();
        let dataset = Arc::clone(&self.dataset);
        
        tokio::spawn(async move {
            if let Err(e) = Self::run_async_pool_worker(
                dataset,
                tx,
                batch_size,
                drop_last,
                pool_config,
                dataset_len,
                cancel,  // NEW: Pass cancel token
            ).await {
                eprintln!("AsyncPoolDataLoader error: {}", e);
            }
        });
        
        // Wrap in cancellation-aware stream
        CancellableStream::new(ReceiverStream::new(rx), self.cancel_token)
    }
}

/// Wrapper that cancels on drop
struct CancellableStream<S> {
    inner: S,
    cancel_token: CancellationToken,
}

impl<S> CancellableStream<S> {
    fn new(inner: S, cancel_token: CancellationToken) -> Self {
        Self { inner, cancel_token }
    }
}

impl<S: Stream + Unpin> Stream for CancellableStream<S> {
    type Item = S::Item;
    
    fn poll_next(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
    ) -> Poll<Option<Self::Item>> {
        Pin::new(&mut self.inner).poll_next(cx)
    }
}

impl<S> Drop for CancellableStream<S> {
    fn drop(&mut self) {
        // Cancel all background tasks when stream is dropped
        self.cancel_token.cancel();
    }
}
```

#### 2.2 Update Worker to Respect Cancellation

```rust
async fn run_async_pool_worker(
    dataset: Arc<MultiBackendDataset>,
    tx: mpsc::Sender<Result<Vec<Vec<u8>>, DatasetError>>,
    batch_size: usize,
    drop_last: bool,
    pool_config: PoolConfig,
    dataset_len: usize,
    cancel: CancellationToken,  // NEW
) -> Result<()> {
    // ... setup ...
    
    while !pending_requests.is_empty() {
        // Check cancellation before processing
        if cancel.is_cancelled() {
            tracing::debug!("Pool worker cancelled, exiting");
            break;
        }
        
        if let Some((index, result)) = pending_requests.next().await {
            // ... process result ...
            
            // Check cancellation before sending
            if cancel.is_cancelled() {
                break;
            }
            
            if tx.send(Ok(current_batch)).await.is_err() {
                // Receiver dropped, stop processing
                break;
            }
        }
    }
    
    Ok(())
}
```

---

### Phase 3: Unified Configuration

#### 3.1 Consolidated Config Structure

**File**: `src/config.rs` (new or existing)

```rust
/// Universal loader configuration
#[derive(Debug, Clone)]
pub struct LoaderConfig {
    /// Batch formation
    pub batch_size: usize,
    pub drop_last: bool,
    
    /// Concurrency control
    pub pool_size: usize,              // Concurrent get() operations
    pub prefetch_batches: usize,       // MPSC channel capacity (backpressure)
    
    /// Timeouts
    pub request_timeout: Duration,     // Per-request timeout
    pub batch_timeout: Duration,       // Batch formation timeout
    
    /// Range engine settings
    pub range_engine: RangeEngineConfig,
    
    /// Ordering policy
    pub ordering: OrderingPolicy,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum OrderingPolicy {
    /// Out-of-order completion (fastest, uses FuturesUnordered)
    Unordered,
    /// In-order completion (uses FuturesOrdered or index heap)
    Ordered,
}

impl Default for LoaderConfig {
    fn default() -> Self {
        Self {
            batch_size: 32,
            drop_last: false,
            pool_size: 16,
            prefetch_batches: 4,
            request_timeout: Duration::from_secs(30),
            batch_timeout: Duration::from_secs(60),
            range_engine: RangeEngineConfig::default(),
            ordering: OrderingPolicy::Unordered,
        }
    }
}
```

#### 3.2 Migrate Existing Code

- `LoaderOptions` ‚Üí `LoaderConfig`
- `PoolConfig` ‚Üí Merged into `LoaderConfig`
- All references updated

---

### Phase 4: Ordering Control

#### 4.1 Add FuturesOrdered Support

```rust
use futures::stream::{FuturesOrdered, FuturesUnordered};

async fn run_async_pool_worker_ordered(
    // Same signature as unordered version
    config: LoaderConfig,
    // ...
) -> Result<()> {
    // Use FuturesOrdered instead of FuturesUnordered
    type RequestFuture = Pin<Box<dyn Future<Output = (usize, Result<Vec<u8>>)> + Send>>;
    let mut pending_requests: FuturesOrdered<RequestFuture> = FuturesOrdered::new();
    
    // Same logic, but results come back in order
    // This makes batch formation deterministic
}
```

**Dispatch based on config**:
```rust
pub fn stream_with_pool(self, config: LoaderConfig) -> ReceiverStream<...> {
    match config.ordering {
        OrderingPolicy::Unordered => {
            // Use existing FuturesUnordered implementation
            self.stream_unordered(config)
        }
        OrderingPolicy::Ordered => {
            // Use new FuturesOrdered implementation
            self.stream_ordered(config)
        }
    }
}
```

---

### Phase 5: Per-Request Timeouts & Retries

#### 5.1 Retry Policy

```rust
#[derive(Debug, Clone)]
pub struct RetryPolicy {
    pub max_retries: usize,
    pub initial_backoff: Duration,
    pub max_backoff: Duration,
    pub jitter: bool,
}

impl Default for RetryPolicy {
    fn default() -> Self {
        Self {
            max_retries: 3,
            initial_backoff: Duration::from_millis(100),
            max_backoff: Duration::from_secs(10),
            jitter: true,
        }
    }
}
```

#### 5.2 Retry Wrapper

```rust
async fn get_with_retry<F, Fut>(
    get_fn: F,
    policy: &RetryPolicy,
    cancel: &Option<CancellationToken>,
) -> Result<Bytes>
where
    F: Fn() -> Fut,
    Fut: Future<Output = Result<Bytes>>,
{
    let mut attempt = 0;
    let mut backoff = policy.initial_backoff;
    
    loop {
        // Check cancellation
        if let Some(token) = cancel {
            if token.is_cancelled() {
                return Err(anyhow::anyhow!("Cancelled"));
            }
        }
        
        match get_fn().await {
            Ok(bytes) => return Ok(bytes),
            Err(e) if attempt >= policy.max_retries => {
                return Err(anyhow::anyhow!("Max retries exceeded: {}", e));
            }
            Err(e) => {
                attempt += 1;
                tracing::warn!("Request failed (attempt {}/{}): {}", 
                    attempt, policy.max_retries + 1, e);
                
                // Calculate backoff with optional jitter
                let delay = if policy.jitter {
                    let jitter = rand::random::<f64>() * 0.3; // ¬±30%
                    backoff.mul_f64(1.0 + jitter)
                } else {
                    backoff
                };
                
                tokio::time::sleep(delay).await;
                
                // Exponential backoff
                backoff = (backoff * 2).min(policy.max_backoff);
            }
        }
    }
}
```

---

## Implementation Timeline

### Week 1: Core Infrastructure (Days 1-3)

**Day 1: Generic RangeEngine**
- [ ] Create `src/range_engine_generic.rs`
- [ ] Implement stream-based download logic
- [ ] Add comprehensive unit tests
- [ ] Benchmark against current S3 implementation

**Day 2: File Backend Integration**
- [ ] Add range engine to FileSystemObjectStore
- [ ] Add configuration threshold
- [ ] Test with various file sizes
- [ ] Benchmark performance improvement

**Day 3: Cancellation Infrastructure**
- [ ] Add `CancellationToken` to DataLoader
- [ ] Create `CancellableStream` wrapper
- [ ] Update pool worker to respect cancellation
- [ ] Test clean shutdown behavior

### Week 2: Expansion & Polish (Days 4-7)

**Day 4: DirectIO Backend**
- [ ] Integrate RangeEngine with DirectIO
- [ ] Handle alignment requirements
- [ ] Test zero-copy behavior
- [ ] Benchmark

**Day 5: Cloud Backends**
- [ ] Integrate RangeEngine with Azure
- [ ] Integrate RangeEngine with GCS
- [ ] Integrate with S3 (replace sharded_client logic)
- [ ] Test all backends

**Day 6: Configuration & Ordering**
- [ ] Create unified `LoaderConfig`
- [ ] Migrate all existing code
- [ ] Implement `FuturesOrdered` path
- [ ] Add retry policies

**Day 7: Testing & CLI Cleanup**
- [ ] Comprehensive integration tests
- [ ] Performance benchmarks (all backends)
- [ ] Remove deprecated `list` command
- [ ] Documentation updates

---

## Success Criteria

### Performance ‚úÖ
- [ ] File backend: 30%+ throughput for files >100MB
- [ ] DirectIO backend: 30%+ throughput for files >100MB
- [ ] Azure backend: 30%+ throughput for files >100MB
- [ ] GCS backend: 30%+ throughput for files >100MB
- [ ] No regression for small files (<4MB)

### Architecture ‚úÖ
- [ ] RangeEngine works with all backends
- [ ] Clean cancellation on stream drop
- [ ] Unified configuration surface
- [ ] Both ordered and unordered modes work
- [ ] Retry policies functional

### Code Quality ‚úÖ
- [ ] Zero compiler warnings
- [ ] All tests pass (Rust + Python)
- [ ] Deprecated commands removed
- [ ] Documentation updated

### User Experience ‚úÖ
- [ ] Zero breaking API changes
- [ ] Transparent performance improvements
- [ ] Consistent behavior across backends

---

## Migration Path for Existing Code

### For Users

**No changes required!** All improvements are internal optimizations.

```python
# v0.9.1 code continues to work
import s3dlio
data = s3dlio.get("s3://bucket/key")

# New performance automatically applied for large files
data = s3dlio.get("s3://bucket/large-file.bin")  # Uses RangeEngine internally
```

### For Developers

**Old RangeEngine** (S3-specific):
```rust
let engine = RangeEngine::new(config);
engine.download_object(uri, size).await?;
```

**New RangeEngine** (generic):
```rust
let engine = RangeEngine::new(config);
let get_range_fn = |offset, length| store.get_range(uri, offset, Some(length));
let (bytes, stats) = engine.download(size, get_range_fn, None).await?;
```

---

## Testing Strategy

### Unit Tests
- RangeEngine with mock backends
- Cancellation token behavior
- Ordering policies
- Retry logic

### Integration Tests
- Each backend with RangeEngine
- Large files (100MB, 1GB)
- Network failures
- Timeout scenarios
- Cancellation mid-download

### Performance Tests
```bash
# Before/after comparison per backend
./scripts/benchmark_backend.sh --backend file --size 1GB --runs 5
./scripts/benchmark_backend.sh --backend s3 --size 1GB --runs 5
./scripts/benchmark_backend.sh --backend azure --size 1GB --runs 5
./scripts/benchmark_backend.sh --backend gcs --size 1GB --runs 5
```

### Stress Tests
- Concurrent loaders
- Stream drop during download
- Network interruptions
- High concurrency (1000+ concurrent ranges)

---

## Dependencies

### New Crates
```toml
[dependencies]
tokio-util = { version = "0.7", features = ["sync"] }  # CancellationToken
# All other dependencies already present
```

### Existing Crates (already used)
- `futures` - Stream combinators
- `tokio` - Async runtime
- `tokio-stream` - ReceiverStream
- `bytes` - Zero-copy buffers
- `anyhow` - Error handling

---

## Release Notes Preview

```markdown
## v0.9.2 - Universal Stream-Based Performance

### Performance üöÄ
- **Universal Range Engine**: All backends now use concurrent range downloads
  - File: 30-50% throughput improvement for files >4MB
  - DirectIO: 30-50% throughput improvement for files >4MB
  - Azure: 30-50% throughput improvement for files >4MB
  - GCS: 30-50% throughput improvement for files >4MB
  - S3: Maintains existing high performance
- Automatic optimization based on file size (configurable threshold)
- Zero user-facing API changes

### Architecture üèóÔ∏è
- Stream-based I/O throughout entire codebase
- Generic RangeEngine decoupled from S3
- Proper cancellation infrastructure (clean shutdown)
- Unified configuration surface (LoaderConfig)
- Ordered and unordered completion modes

### Reliability üõ°Ô∏è
- Cancellation tokens prevent resource leaks
- Per-request timeouts with exponential backoff
- Retry policies for transient failures
- Graceful degradation on errors

### CLI üßπ
- Removed deprecated `list` command
- Universal `ls` command works with all backends
- Consistent interface across storage systems

### Code Quality ‚ú®
- Reduced S3-specific code paths
- Eliminated backend duplication
- Zero compiler warnings
- Comprehensive test coverage

### Breaking Changes
None - fully backward compatible
```

---

## Next Steps

Ready to start implementation? Recommended order:

1. **Start here**: Create generic RangeEngine (Day 1)
2. **Prove concept**: Integrate with File backend (Day 2)
3. **Add safety**: Cancellation infrastructure (Day 3)
4. **Scale out**: Other backends (Days 4-5)
5. **Polish**: Config unification, CLI cleanup (Days 6-7)

Shall we begin with the generic RangeEngine implementation?
