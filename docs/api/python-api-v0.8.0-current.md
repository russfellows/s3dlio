# S3DLIO Python API Reference - Version 0.8.0 (Current)

**âš ï¸ IMPORTANT VERSION INFORMATION**
- **This document**: For s3dlio v0.8.0+ (September 2025)
- **Previous API**: See `python_api_legacy_v0.7.md` for older versions
- **Status**: This is the CURRENT and RECOMMENDED API

---

## ğŸ†• **NEW Enhanced API (v0.8.0+)** - **âœ… WORKING**

The enhanced API provides a unified interface across all storage backends with generic factory functions.

### Core Functions (âœ… WORKING)

#### `create_dataset(uri: str, options: dict = None) -> PyDataset`
**Status**: âœ… **FULLY WORKING** - Main dataset creation function

```python
import s3dlio

# âœ… WORKING: File system dataset
dataset = s3dlio.create_dataset("file:///path/to/data")

# âœ… WORKING: S3 dataset (requires AWS credentials)
dataset = s3dlio.create_dataset("s3://bucket/prefix/")

# âœ… WORKING: With options
dataset = s3dlio.create_dataset("file:///data", {
    "batch_size": 32,
    "shuffle": True
})
```

#### `create_async_loader(uri: str, options: dict = None) -> PyBytesAsyncDataLoader`
**Status**: âœ… **FULLY WORKING** - Async data loading

```python
import s3dlio
import asyncio

# âœ… WORKING: Create async loader
loader = s3dlio.create_async_loader("file:///path/to/data")

# âœ… WORKING: Async iteration
async def process():
    async for item in loader:
        print(f"Got {len(item)} bytes")

asyncio.run(process())
```

### Core Classes (âœ… WORKING)

#### `PyDataset` 
**Status**: âœ… **AVAILABLE** - Generic dataset container

```python
# âœ… WORKING: Dataset creation returns PyDataset
dataset = s3dlio.create_dataset("file:///data")
print(type(dataset))  # <class 'builtins.PyDataset'>
```

#### `PyBytesAsyncDataLoader`
**Status**: âœ… **AVAILABLE** - Async data loader

```python
# âœ… WORKING: Async loader creation returns PyBytesAsyncDataLoader
loader = s3dlio.create_async_loader("file:///data")
print(type(loader))  # <class 'builtins.PyBytesAsyncDataLoader'>
```

### URI Schemes (âœ… WORKING)

#### File System (`file://`) - âœ… **FULLY WORKING**
```python
# âœ… WORKING: Single file
s3dlio.create_dataset("file:///home/user/data.txt")

# âœ… WORKING: Directory (recursive scan)
s3dlio.create_dataset("file:///home/user/dataset/")

# âœ… WORKING: Network paths
s3dlio.create_dataset("file:///mnt/vast1/data/")
```

#### Amazon S3 (`s3://`) - âœ… **WORKING** (requires credentials)
```python
# âœ… WORKING: S3 prefix
s3dlio.create_dataset("s3://bucket/prefix/")

# âœ… WORKING: Single S3 object
s3dlio.create_dataset("s3://bucket/object.bin")
```

#### Azure (`az://`) - ğŸš§ **FRAMEWORK READY**
```python
# ğŸš§ FRAMEWORK EXISTS: Will be completed in future release
s3dlio.create_dataset("az://container/prefix/")
```

#### Direct I/O (`direct://`) - ğŸš§ **FRAMEWORK READY**  
```python
# ğŸš§ FRAMEWORK EXISTS: Will be completed in future release
s3dlio.create_dataset("direct:///dev/nvme0n1")
```

---

## ğŸ”§ **Legacy API (v0.7.x and earlier)** - **âš ï¸ MIXED STATUS**

These functions are preserved for backward compatibility but have varying status.

### Core Functions - **Status Varies**

#### `get(uri: str) -> bytes` - âœ… **WORKING**
```python
# âœ… WORKING: Get single object
data = s3dlio.get("s3://bucket/object.txt")
```

#### `put(uri: str, data: bytes)` - âœ… **WORKING**
```python
# âœ… WORKING: Put single object
s3dlio.put("s3://bucket/object.txt", b"data")
```

#### `list(uri: str) -> List[str]` - âœ… **WORKING**
```python  
# âœ… WORKING: List objects
keys = s3dlio.list("s3://bucket/prefix/")
```

#### `stat(uri: str) -> dict` - âœ… **WORKING**
```python
# âœ… WORKING: Get object metadata
metadata = s3dlio.stat("s3://bucket/object.txt")
```

### PyTorch Integration

#### `S3IterableDataset` - **ğŸ› FIXED in v0.8.0**
```python
from s3dlio.torch import S3IterableDataset

# âœ… NOW WORKING: Bug fixed in v0.8.0
# Previously failed with "PyS3AsyncDataLoader not found"
dataset = S3IterableDataset("file:///data/")

# âœ… WORKING: PyTorch DataLoader integration
from torch.utils.data import DataLoader
dataloader = DataLoader(dataset, batch_size=32)
```

**Bug History**:
- **v0.7.x and earlier**: âŒ Failed with "PyS3AsyncDataLoader not found" 
- **v0.8.0+**: âœ… Fixed - now uses `create_async_loader` internally

### Legacy Classes - **âš ï¸ DEPRECATED or PROBLEMATIC**

#### `PyS3Dataset` - âš ï¸ **DEPRECATED**
```python
# âš ï¸ DEPRECATED: Use create_dataset() instead
# May still work but not recommended
if hasattr(s3dlio, 'PyS3Dataset'):
    old_dataset = s3dlio.PyS3Dataset("s3://bucket/data/")
```

#### `PyS3AsyncDataLoader` - âŒ **WAS BROKEN, NOW FIXED**
```python
# âŒ v0.7.x: This class didn't exist, causing PyTorch errors
# âœ… v0.8.0+: Replaced with generic create_async_loader()

# ğŸš« DON'T USE: This was the source of the original bug
# âœ… USE INSTEAD: s3dlio.create_async_loader()
```

---

## ğŸ§ª **What Actually Works - Test Status**

### âœ… **Verified Working Functions**

Based on actual testing, these functions are confirmed working:

```python
import s3dlio

# âœ… CONFIRMED: These functions exist and work
create_dataset = getattr(s3dlio, 'create_dataset')        # âœ… Works
create_async_loader = getattr(s3dlio, 'create_async_loader')  # âœ… Works

# âœ… CONFIRMED: Legacy functions work  
get_func = getattr(s3dlio, 'get')                        # âœ… Works
put_func = getattr(s3dlio, 'put')                        # âœ… Works
list_func = getattr(s3dlio, 'list')                      # âœ… Works
stat_func = getattr(s3dlio, 'stat')                      # âœ… Works
```

### âš ï¸ **Known Issues**

#### Linter vs Runtime Discrepancy
```python
# âš ï¸ LINTER ISSUE: Shows as "not found" but actually works
# This is a linter/IDE problem, not a runtime problem
dataset = s3dlio.create_dataset("file:///data")  # âœ… Actually works!
```

#### PyDataset Iteration  
```python
# ğŸš§ NEEDS INVESTIGATION: Direct iteration may have issues
dataset = s3dlio.create_dataset("file:///data")
# This may not work: for item in dataset
# But the dataset object is created successfully
```

#### S3IterableDataset Parameters
```python  
# âš ï¸ PARAMETER ISSUE: May require loader_opts parameter
from s3dlio.torch import S3IterableDataset
# This might need: S3IterableDataset("uri", loader_opts={})
```

---

## ğŸ“‹ **Migration Checklist**

### From v0.7.x to v0.8.0

#### âœ… **No Changes Needed** (Backward Compatible)
- All existing `get()`, `put()`, `list()`, `stat()` calls work unchanged
- Existing S3IterableDataset usage now works (bug was fixed internally)

#### ğŸ†• **New Capabilities Available**
- Use `create_dataset()` for unified dataset creation
- Use `create_async_loader()` for async data processing  
- File system support with `file://` URIs
- Multi-backend support ready for expansion

#### âš ï¸ **Recommended Updates**
```python
# OLD (still works)
data = s3dlio.get("s3://bucket/object")

# NEW (recommended for consistency)
dataset = s3dlio.create_dataset("s3://bucket/object")

# OLD PyTorch usage (now works, was broken)
from s3dlio.torch import S3IterableDataset
dataset = S3IterableDataset("s3://bucket/data/")  # Now works!

# NEW PyTorch usage (also works)
loader = s3dlio.create_async_loader("s3://bucket/data/")
```

---

## ğŸ” **Error Handling**

### Working Error Detection
```python
# âœ… WORKING: Error handling is functional
try:
    dataset = s3dlio.create_dataset("ftp://invalid/scheme")
except RuntimeError as e:
    print(f"Expected error: {e}")  # Properly caught

try:
    dataset = s3dlio.create_dataset("file:///nonexistent/path")  
except RuntimeError as e:
    print(f"Path error: {e}")  # Properly caught
```

### Common Error Types
- `RuntimeError`: Invalid URIs, missing paths, backend issues
- `Exception`: General configuration or parameter problems

---

## ğŸ“š **Additional Documentation**

- **Enhanced API Reference**: `enhanced-api-v0.8.0.md` - Complete technical details
- **Migration Guide**: `migration-guide-v0.8.0.md` - Detailed migration instructions  
- **Legacy API Reference**: `python_api_legacy_v0.7.md` - For older versions
- **Examples**: `examples/enhanced_api_examples.py` - Working code examples

---

## ğŸ·ï¸ **Version History**

- **v0.8.0** (September 2025): Enhanced API, bug fixes, multi-backend support
- **v0.7.x** (Earlier): Original API, PyTorch bug present  
- **v0.6.x and earlier**: See legacy documentation

---

**ğŸ’¡ RECOMMENDATION**: Use the new enhanced API (`create_dataset`, `create_async_loader`) for new projects while existing code continues to work unchanged.