# s3dlio Changelog

## Version 0.7.8 - Rust API Cleanup & O_DIRECT Implementation (August 28, 2025)

This release delivers a **complete Rust API redesign** with clean, stable interfaces for external developers and a **working O_DIRECT implementation** with streaming support. The new API provides better ergonomics, backward compatibility, and enterprise-ready documentation.

### ðŸŽ¯ **Clean Stable Rust API (NEW)**
- **Public API Module**: New `s3dlio::api` module with stable, documented interfaces for external developers
- **Factory Functions**: `store_for_uri()` and `direct_io_store_for_uri()` for easy object store creation
- **ObjectStore Trait**: Unified interface with `put()`, `get()`, and streaming operations across all backends
- **Backward Compatibility**: Existing code continues to work unchanged with new APIs available alongside

### âš¡ **O_DIRECT Implementation**
- **Direct I/O Support**: Linux O_DIRECT implementation with 4KB alignment for maximum performance
- **Streaming Writer**: `DirectIOWriter` with `write_chunk()` and `finalize()` methods for large uploads
- **Hybrid I/O**: Automatic switching between O_DIRECT (aligned data) and buffered I/O (unaligned final chunks)
- **Error Handling**: Fixed finalization crashes and implemented proper resource cleanup

### ðŸ“š **Comprehensive Documentation**
- **API Design Guide**: Complete documentation of design principles and usage patterns
- **User Guide**: Step-by-step examples for common use cases and advanced scenarios
- **Future Work**: Documented remaining O_DIRECT data persistence optimization tasks
- **Examples**: Added `rust_api_basic_usage.rs` and comprehensive O_DIRECT test examples

### ðŸ”§ **Developer Experience**
- **Stable Interface**: Clear separation between stable `api` and internal `api::advanced` modules
- **Configuration Types**: `WriterOptions` and other configuration structs in public API
- **Error Propagation**: Consistent error handling across all API methods
- **Test Coverage**: Extensive examples validating O_DIRECT functionality and hybrid I/O

### ðŸ—ï¸ **Code Organization**
- **API Module**: New `src/api.rs` and `src/api/` directory with clean public interfaces
- **Documentation**: Organized in `docs/api/` and `docs/development/` directories
- **Examples**: Clear demonstration code showing best practices and usage patterns
- **Future Tasks**: Documented path forward for completing O_DIRECT data persistence optimization

## Version 0.7.7 - Phase 2 Streaming API & Complete Python Bindings (August 28, 2025)

This release delivers the **complete Phase 2 streaming infrastructure** with production-ready Python bindings, comprehensive test coverage, and full multi-backend support. Introduces powerful streaming writer APIs enabling memory-efficient uploads across all storage backends with zero-copy optimizations and optional compression.

### ðŸš€ **Phase 2 Streaming API (NEW)**
- **Universal Streaming Writers**: `ObjectWriter` trait with `write_chunk()`, `write_owned_bytes()`, and `finalize()` methods
- **Multi-Backend Support**: S3, Azure Blob Storage, Filesystem, and Direct I/O filesystem streaming
- **Zero-Copy Optimization**: `write_owned_bytes()` method eliminates memory copies for maximum performance
- **Compression Integration**: Optional zstd compression with automatic `.zst` extension handling

### ðŸ **Complete Python Bindings**
- **PyWriterOptions**: Full configuration control with compression, buffer sizes, and upload parameters
- **PyObjectWriter**: Python wrapper with synchronous API for seamless integration
- **Creator Functions**: `create_s3_writer()`, `create_azure_writer()`, `create_filesystem_writer()`, `create_direct_filesystem_writer()`
- **Post-Finalization Stats**: Access to `bytes_written()` and `compressed_bytes()` even after writer finalization

### âœ… **Comprehensive Test Coverage**
- **Rust Test Suite**: 7 streaming backend tests covering all writer functionality and compression
- **Python Test Suite**: 8 complete test scenarios validating all streaming operations
- **Error Handling**: Robust validation of edge cases, finalization states, and invalid operations
- **Compression Validation**: File extension handling (`.zst`) and compression ratio verification

### ðŸ—ï¸ **Code Organization & Cleanup**
- **Modular Structure**: All Python APIs properly organized in `src/python_api/` subdirectory
- **Test Organization**: All Python tests moved to `python/tests/` directory following best practices
- **Duplicate Removal**: Eliminated old duplicate files and consolidated streaming functionality
- **Example Code**: Added `examples/phase2_streaming_demo.rs` demonstrating streaming API usage

### ðŸ”§ **Infrastructure Improvements**
- **Build System**: Updated maturin integration with proper feature flags
- **Project Structure**: Reorganized scripts to `scripts/` directory for better organization
- **Benchmark Suite**: Added `benches/s3_microbenchmarks.rs` for performance validation
- **Docker Support**: Enhanced Docker configuration for streaming functionality

### ðŸ“Š **Performance Features**
- **Streaming Memory Efficiency**: Write arbitrarily large files with minimal memory footprint
- **Async Writer Creation**: Non-blocking writer instantiation across all backends
- **Synchronous Write Operations**: Optimized for Python integration with `block_on` pattern
- **Statistics Preservation**: Maintain access to upload metrics after stream completion

## Version 0.7.6 - Advanced Performance Profiling Infrastructure (August 27, 2025)

This release introduces **comprehensive performance profiling capabilities** with CPU sampling, flamegraph generation, and advanced benchmarking infrastructure. Validated with large-scale testing achieving **1.88 GB/s upload** and **5.34 GB/s download** throughput.

### ðŸ”¥ **Performance Profiling Infrastructure (NEW)**
- **CPU Profiling**: High-frequency sampling (100Hz) with `pprof` integration for detailed performance analysis
- **Flamegraph Generation**: Visual CPU hotspot analysis with SVG output to `profiles/` directory
- **Zero-Overhead Design**: Feature-gated profiling (`--features profiling`) with no performance impact when disabled
- **Real S3 Integration**: Profiling with actual AWS S3 operations using `.env` configuration

### ðŸ“Š **Comprehensive Benchmarking Suite**
- **Criterion Microbenchmarks**: Buffer operations (25 GB/s), data generation (2.6 GB/s), vector ops, URI parsing
- **Large-Scale Testing**: 5GB+ multi-object workloads with realistic 2-8MB object sizes
- **Complete S3 Lifecycle**: Bucket creation â†’ upload â†’ listing â†’ download â†’ cleanup â†’ deletion
- **Performance Validation**: Demonstrated **1,879 MB/s upload** and **5,335 MB/s download** speeds

### ðŸŽ¯ **Advanced Profiling Features**
- **Async Task Monitoring**: Tokio console integration (`tokio-console`) for runtime analysis
- **Structured Tracing**: Context-aware logging with span hierarchy and performance metrics
- **Multiple Profiling Modes**: Section profiling, full-application profiling, and targeted micro-profiling
- **Visual Analysis**: Detailed flamegraphs showing CPU distribution across concurrent operations

### ðŸ—‚ï¸ **Project Structure Optimization**
- **Documentation Organization**: Restructured `docs/` into `performance/`, `development/`, and `api/` subdirectories
- **Clean Example Structure**: Consolidated profiling examples with shell script integration
- **Output Organization**: Dedicated `profiles/` directory for generated flamegraphs and analysis
- **Professional Structure**: Following Rust best practices for scalable project organization

### ðŸ”§ **New Profiling Examples & Tools**
- **`large_scale_s3_test.rs`**: Comprehensive 5GB+ stress testing with concurrent operations
- **`simple_flamegraph_test.rs`**: Basic profiling validation and flamegraph generation
- **`profile_s3_operations.rs`**: Real S3 operation profiling with environment integration
- **`profile_performance.sh`**: Automated profiling script for continuous performance monitoring

### ðŸ“ˆ **Performance Results & Insights**
- **Upload Performance**: 1.88 GB/s sustained throughput with 16 concurrent operations
- **Download Performance**: 5.34 GB/s with optimized 32 concurrent range requests
- **Memory Efficiency**: Constant memory usage regardless of dataset size
- **CPU Efficiency**: Minimal overhead allowing resources for ML workloads

For detailed performance analysis and flamegraph results, see: [Performance Profiling Results](performance/Profiling_Results_Summary.md)

## Version 0.7.5 - HTTP Client Optimization & Performance Enhancement (August 27, 2025)

This release delivers **advanced HTTP client optimization** through strategic AWS SDK integration, providing measurable performance improvements with full backward compatibility and environment variable control.

### ðŸš€ **HTTP Client Performance Optimization (COMPLETE)**
- **AWS SDK Fork Integration**: Successfully forked `aws-smithy-http-client` v1.1.0 to expose connection pool configuration
- **Performance Improvement**: +2-3% throughput improvement with optimized HTTP client configuration
- **Baseline Improvement**: +144% performance improvement over original repository baseline (1.9 GB/s â†’ 4.8 GB/s)
- **Environment Variable Control**: `S3DLIO_USE_OPTIMIZED_HTTP=true` enables optimization while preserving AWS SDK defaults

### âš™ï¸ **Advanced Connection Pool Configuration**
- **Conservative Defaults**: 200 max connections per host (vs default ~100) to avoid resource contention
- **Timeout Optimization**: 800ms idle timeout optimized for 8MB objects based on storage speed analysis
- **HTTP/2 Enhancements**: Adaptive windows with optimized keep-alive settings for better connection reuse
- **Configurable Parameters**: Environment variables for fine-tuning connection pool behavior

### ðŸ”§ **Runtime & Reliability Improvements**
- **Tokio Runtime Fix**: Resolved nested runtime panics with sophisticated context detection and fallback handling
- **Global Runtime Optimization**: Enhanced runtime thread management with intelligent CPU-based defaults
- **Error Handling**: Improved error reporting and graceful fallback mechanisms
- **AWS SDK Compatibility**: Updated to latest AWS SDK behavior version (v2025_08_07)

### ðŸ“Š **Performance Results & Analysis**
- **Measurements**: AWS SDK Default: ~4.7 GB/s, Optimized: ~4.8-4.9 GB/s
- **Infrastructure Sensitivity**: Performance testing revealed storage system throttling effects
- **Production Ready**: Conservative optimization approach ensures reliability in production environments
- **A/B Testing**: Environment variable toggle enables easy performance comparison

### ðŸ› ï¸ **Technical Implementation**
```toml
# Enhanced Cargo.toml with forked dependency
[dependencies]
aws-smithy-http-client = { path = "fork-patches/aws-smithy-http-client" }
```

### ðŸ“– **Documentation & Testing**
- **Updated Documentation**: Enhanced Phase 1 optimization plan with implementation details
- **Comprehensive Testing**: All 84 tests passing across unit, integration, and performance suites
- **Usage Examples**: Clear environment variable configuration and tuning guidance
- **Production Recommendations**: Guidelines for optimal configuration in different environments

---

## Version 0.7.3 - Modular Python API Architecture (August 26, 2025)

This release delivers a **major architectural refactoring** of the Python API, transforming a monolithic 1883-line file into a clean modular structure with comprehensive quality improvements and robust testing infrastructure.

### ðŸ—ï¸ **Modular Architecture Transformation (COMPLETE)**
- **Modular Structure**: Split monolithic `python_api.rs` into organized module directory:
  - `python_core_api.rs` - Core storage operations and basic functionality
  - `python_aiml_api.rs` - NumPy integration, ML datasets, and AI/ML workflows  
  - `python_advanced_api.rs` - Checkpointing, compression, and advanced features
- **Clean Registration**: Main `python_api.rs` now serves as clean orchestrator
- **Separation of Concerns**: Each module handles distinct functional areas
- **Maintainable Codebase**: Easier navigation, debugging, and future enhancements

### ðŸ”§ **Quality & Bug Fixes (COMPLETE)**
- **Zero Warnings**: Eliminated ALL 14 compiler warnings through careful analysis
- **Real Bug Fixes**: Fixed critical logic bug in `opts_from_dict` parameter handling
- **Enhanced API**: Added missing `__len__` and `__getitem__` methods to `PyVecDataset`
- **Signature Corrections**: Fixed `PyAsyncDataLoader` with proper optional parameters
- **Code Quality**: Improved error handling and parameter validation

### ðŸ§ª **Comprehensive Testing Infrastructure (COMPLETE)**
- **Regression Test Suite**: Added 16 comprehensive test cases covering all functionality
- **API Validation**: Tests verify all 49 public functions/classes maintain compatibility
- **Automated Testing**: Created `run_regression_tests.sh` for CI/CD integration
- **Coverage Analysis**: Complete validation of core, AI/ML, and advanced features
- **Continuous Quality**: All tests passing with robust error reporting

### ðŸ“¦ **Enhanced Development Experience**
- **Improved Organization**: Cleaner code structure for easier contributions
- **Better Documentation**: Enhanced inline documentation and module separation
- **Faster Builds**: Modular compilation allows for more efficient development cycles
- **Future-Ready**: Architecture supports easy addition of new features

### âœ… **Validation Results**
```bash
# All functionality preserved and validated
./python/tests/run_regression_tests.sh
# âœ… 16/16 tests passed
# âœ… 49 public functions/classes available
# âœ… Core storage operations working
# âœ… NumPy integration functional
# âœ… Checkpointing and compression working
# âœ… Zero compiler warnings
```

### ðŸŽ¯ **Backward Compatibility**
- **API Preserved**: All existing Python code continues to work unchanged
- **Import Compatibility**: All `import s3dlio` statements work identically
- **Feature Parity**: Every function and class maintains exact same behavior
- **Zero Breaking Changes**: Seamless upgrade path for all users

---

## Version 0.7.2 - Complete Python Compression Integration (August 26, 2025)

This release completes the **full end-to-end Python compression integration** with functional save/load cycle, automatic decompression, and manifest system integration. Python users can now enjoy seamless compression with significant storage savings.

### ðŸŽ¯ **Complete Python Compression Integration (COMPLETE)**
- **Functional Python API**: `compression_level` parameter now fully functional in `PyCheckpointStore`
- **End-to-End Save/Load**: Complete cycle with automatic compression on save and decompression on load
- **Universal Backend Support**: Python compression works across all 4 backends (FileSystem, DirectIO, S3, Azure)
- **Automatic File Management**: Compressed files automatically get `.zst` extensions
- **Manifest Integration**: Manifests correctly record compressed filenames for accurate loading
- **Data Integrity**: All compressed data loads correctly and matches original data
- **Excellent Compression**: Achieved 99.8% compression on repetitive data (10KB â†’ 18 bytes)

### ðŸ”§ **Technical Implementation**
- **CheckpointConfig Enhancement**: Added compression field and `with_compression()` method
- **Writer Enhancement**: Added compression support to all Writer methods (`put_shard`, `get_shard_writer`)
- **ObjectStore Extension**: Added `get_writer_with_compression()` method to all backends
- **Reader Enhancement**: Automatic detection and decompression of `.zst` files
- **Manifest Accuracy**: ShardMeta now records actual compressed filenames with extensions

### âœ… **Test Results**
```python
# This now works perfectly end-to-end!
store = PyCheckpointStore("file:///tmp/test", compression_level=9)
store.save(100, 1, "test", data, None)   # âœ… Compresses: 2600 bytes â†’ 18 bytes  
loaded = store.load_latest()             # âœ… Decompresses correctly
assert loaded == data                    # âœ… Data integrity preserved
```

### ðŸ“ˆ **Performance Benefits**
- **Storage Savings**: Up to 99.8% reduction in file sizes for repetitive data
- **Configurable Levels**: Compression levels 1-22 for speed vs. compression tradeoffs
- **Universal Support**: Works across all storage backends without code changes

---

## Version 0.7.1 - Universal Compression Backend + Python API Enhancements (August 26, 2025)

This release delivers **universal compression across all backends** with accurate reporting, enhanced Python API integration, and comprehensive testing framework improvements.

### ðŸŽ¯ **Universal Compression Backend (COMPLETE)**
- **All Backend Support**: Zstd compression now working across all 4 backends:
  - FileSystemWriter with streaming compression
  - DirectIOWriter with O_DIRECT + compression  
  - S3BufferedWriter with multipart + compression
  - AzureBufferedWriter with streaming + compression
- **Configurable Levels**: Full support for compression levels 1-22
- **Automatic Extensions**: Proper .zst file extension handling
- **Accurate Reporting**: **FIXED** compression ratio calculation (was showing incorrect 1.000 values)
- **Excellent Performance**: Achieved compression ratios:
  - Highly compressible data: **99.9%** compression (50KB â†’ 66 bytes)
  - JSON-like structured data: **88.1%** compression (50KB â†’ 5.9KB)
  - Random-like data: **96.7%** compression (50KB â†’ 1.7KB)
  - High-entropy data: **99.4%** compression (50KB â†’ 276 bytes)

### ðŸ **Python API Enhancements**
- **Compression Parameter**: Added `compression_level` parameter to `PyCheckpointStore`
- **Enhanced Validation**: Complete Python test suite for JAX/PyTorch/TensorFlow frameworks
- **API Stability**: Fixed method signatures and parameter handling
- **Error Handling**: Improved error messages and parameter validation
- **Backward Compatibility**: All existing Python code continues to work

### ðŸ§ª **Enhanced Testing Framework**
- **Comprehensive Coverage**: Added 7/7 passing compression tests across all backends
- **Varied Data Patterns**: Testing with highly compressible, JSON-like, random, and high-entropy data
- **Performance Analysis**: Detailed compression ratio reporting and performance metrics
- **Python Validation**: Complete framework compatibility testing (JAX, PyTorch, TensorFlow)
- **Cross-Platform**: Validated on FileSystem, DirectIO, S3, and Azure backends

### ðŸ”§ **Bug Fixes & Improvements**
- **Fixed**: Compression ratio reporting showing incorrect 1.000 instead of actual ratios
- **Fixed**: Python API method signature issues and parameter handling
- **Fixed**: Streaming compression buffer management in all writers
- **Improved**: Test methodology using actual file sizes for accurate reporting
- **Enhanced**: Error handling and edge case management

### ðŸ“š **Documentation & Development**
- **API Versioning**: Preserved multiple versions of `python_api.rs` for development continuity
  - `python_api.rs` - Current v0.7.1 with compression support
  - `python_api.rs.current` - Working baseline (pre-compression)  
  - `python_api.rs.git_previous` - Original reference version
- **Comprehensive Docs**: Added implementation status and Python API changes analysis
- **Development Guide**: Created `docs/python_api_versions_README.md` for version management

### âš ï¸ **Known Limitations**
- **Python Compression**: Parameter wired but needs CheckpointConfig integration for full functionality
- **NumPy Functions**: `save_numpy_array()`, `load_numpy_array()`, and `PyValidatedCheckpointReader` temporarily disabled due to PyO3/numpy compatibility issues
- **Manifest Loading**: Reader manifest detection needs enhancement

### ðŸ“Š **Production Readiness Status**
- âœ… **Rust Compression Backend**: Production ready across all 4 backends
- âœ… **Python Core API**: Production ready for distributed checkpointing workflows
- âœ… **Cross-Framework Support**: JAX, PyTorch, TensorFlow compatibility validated
- âš ï¸ **Python Compression**: Architecture ready, needs final integration
- âŒ **Advanced NumPy**: Requires PyO3/numpy compatibility work

### ðŸ”„ **Migration Notes**
- All existing Rust code continues to work without changes
- Python code using `PyCheckpointStore` may optionally add `compression_level` parameter
- No breaking changes to existing APIs
- Enhanced compression available immediately in Rust, coming soon to Python

---

## Version 0.7.0 - Phase 3 Complete: Compression, Integrity, Python Exchange + Azure Ungating (August 26, 2025)

This major release completes **Phase 3 Implementation** with all priorities (2, 3, 4) plus Azure ungating, providing enterprise-grade compression, integrity validation, enhanced Python integration, and seamless multi-cloud operations.

### ðŸŽ¯ Phase 3 Priority 2: Compression Support
- **Zstd Compression**: Full streaming compression integration with configurable levels (1-22)
- **FileSystemWriter**: Complete compression implementation with automatic file extension handling (.zst)
- **Performance**: Optimized streaming compression maintaining zero-copy semantics where possible
- **Backend Coverage**: Compression support across all ObjectStore implementations

### ðŸ”’ Phase 3 Priority 3: Integrity Validation
- **CRC32C Validation**: Advanced integrity validation with corruption detection
- **Range Validation**: Partial read validation for optimized data access patterns
- **Concurrent Validation**: High-performance validation supporting concurrent operations
- **Error Reporting**: Detailed corruption detection with actionable error messages

### ðŸ Phase 3 Priority 4: Python-Rust Exchange Enhancement
- **Tensor Data Exchange**: Enhanced zero-copy tensor serialization/deserialization
- **Metadata Preservation**: Complete metadata retention during Python-Rust data exchange
- **Distributed Checkpoints**: Advanced checkpoint loading with validation
- **Error Handling**: Improved error propagation between Python and Rust layers

### â˜ï¸ Azure Ungating
- **Always Available**: Azure ObjectStore no longer requires feature flags
- **Unified Factory**: `store_for_uri()` supports all backends (File, S3, Azure) seamlessly
- **Multi-Cloud Ready**: Complete backend parity for enterprise multi-cloud deployments

### ðŸ§ª Comprehensive Testing
- **Phase 3 Tests**: 17/17 tests passing across all priorities
- **Backend Parity**: 2/2 tests confirming complete ObjectStore trait implementation
- **Zero Warnings**: Clean compilation with all compiler warnings resolved

### ðŸ”§ Technical Improvements
- **FileSystemWriter**: Added `new_with_compression()`, compression fields, and zstd integration
- **ObjectStore Trait**: Enhanced with validation methods and compression support
- **Test Coverage**: Comprehensive validation of compression levels, integrity checks, and Python exchange

## Version 0.6.2 - Phase 3 Enhanced Metadata with Checksum Integration (August 26, 2025)

This release completes **Phase 3 Priority 1**, implementing comprehensive checksum integration across the s3dlio zero-copy streaming infrastructure to provide data integrity validation for all storage backends.

### ðŸ” Enhanced Metadata with Checksum Integration

- **Comprehensive Checksum Support**: Added CRC32C checksum computation to all ObjectWriter implementations
- **Multi-Backend Consistency**: Checksums work identically across FileSystem, DirectIO, S3, and Azure storage backends
- **ObjectWriter Enhancement**: Extended trait with `checksum() -> Option<String>` method returning `"crc32c:xxxxxxxx"` format
- **Zero-Copy Maintained**: Checksum computation preserves zero-copy semantics with minimal overhead (~1-2% CPU)
- **Checkpoint Integration**: ShardMeta now contains computed checksums instead of hardcoded None values

### ðŸ› ï¸ Technical Implementation

- **Core Enhancement**: Enhanced ObjectWriter trait in `src/object_store.rs`
- **Backend Updates**: Updated all storage writers (FileSystemWriter, DirectIOWriter, S3BufferedWriter, AzureBufferedWriter)
- **Checkpoint System**: Added `finalize_writer_to_shard_meta()` and `finalize_shard_meta_with_checksum()` helper methods
- **Performance**: CRC32C algorithm with SIMD acceleration, incremental updates during streaming
- **Memory Efficiency**: Peak memory = single chunk size, checksums don't break streaming architecture

### ðŸ§ª Comprehensive Testing

- **New Test Suites**: 8 new tests validating checksum computation, consistency, and checkpoint integration
- **Coverage**: Unit tests (`test_phase3_checksums.rs`) and integration tests (`test_checkpoint_checksums.rs`)
- **Regression Testing**: All 27 existing tests continue to pass, ensuring backward compatibility
- **Multi-Backend Validation**: Checksum consistency verified across all storage backends

### ðŸ“Š Performance Impact

- **CPU Overhead**: ~1-2% increase for checksum computation
- **Memory Usage**: Negligible additional memory (~32 bytes per writer)
- **I/O Performance**: No impact on streaming throughput
- **Scalability**: Handle checkpoints larger than available RAM with integrity validation

### ðŸ” Data Integrity Features

- **Incremental Checksums**: Updates during each `write_chunk()` call
- **Format Standardization**: All checksums use `"crc32c:xxxxxxxx"` format
- **Validation Ready**: Infrastructure prepared for read-time integrity validation
- **Checkpoint Metadata**: Manifest files now include actual computed checksums

This release provides the foundation for advanced integrity validation and sets the stage for Phase 3 Priorities 2-4 (compression support, advanced integrity validation, and enhanced Python-Rust data exchange).

---

## Version 0.6.1 - Zero-Copy Streaming Infrastructure

Enhanced the checkpointing system with comprehensive zero-copy streaming capabilities, enabling memory-efficient processing of arbitrarily large checkpoints. This major advancement eliminates memory bottlenecks and enables checkpointing of models larger than available RAM.

### ðŸš€ Phase 2 Streaming Features
- **Zero-Copy Streaming**: Direct memory transfer from Python to Rust without copying
- **ObjectWriter Trait**: Unified streaming interface across all storage backends
- **Memory Optimization**: Peak memory usage = single chunk size (not total checkpoint size)
- **Incremental Writing**: Process large checkpoints without full buffering
- **Python Streaming API**: `PyCheckpointStream` for memory-efficient data transfer

### Memory Efficiency Examples
```python
# Traditional approach: Memory usage = full model size
shard_meta = writer.save_distributed_shard(step, epoch, framework, data)

# New streaming approach: Memory usage = single chunk size
stream = writer.get_distributed_shard_stream(step, epoch, framework)
for chunk in data_chunks:
    stream.write_chunk(chunk)  # Zero-copy transfer
shard_meta = stream.finalize()

# Result: 99%+ reduction in peak memory usage for large models
```

### Performance Benefits
- **50GB Model**: Traditional = 50GB peak memory, Streaming = 64MB peak memory
- **Scalability**: Handle checkpoints larger than available RAM  
- **Zero Bottlenecks**: Eliminates memory-based performance limitations
- **Backend Consistency**: Streaming works identically across S3, Azure, file://, and DirectIO

ðŸ“– **[Enhanced Checkpoint Documentation](Checkpoint_Features.md)** - Complete streaming API guide with memory efficiency comparisons

---

## Version 0.6.0 - Comprehensive Checkpointing System

Added a complete distributed checkpointing system modeled after the AWS S3 PyTorch Connector, providing high-performance checkpoint operations across all storage backends. The checkpoint system is implemented primarily in Rust for optimal performance and safety, with Python bindings for seamless integration with ML frameworks including PyTorch, JAX, and TensorFlow.

### Key Features
- **Multi-Backend Checkpointing**: Full support across S3, Azure Blob, file://, and O_DIRECT backends
- **Distributed Coordination**: Multi-rank checkpoint coordination with manifest-based discovery
- **Framework Integration**: Native support for PyTorch, JAX, and TensorFlow workflows
- **Storage Optimization**: Hot-spot avoidance strategies (Flat, RoundRobin, Binary) for cloud storage
- **Production Ready**: Comprehensive test coverage with 28 tests across Rust and Python

### API Examples
```rust
// Rust API
let store = CheckpointStore::open("s3://bucket/checkpoints")?;
let writer = store.writer(world_size, rank)?;
let manifest_key = writer.write_manifest(epoch, "pytorch", shard_metas, None).await?;
```

```python
# Python API
import s3dlio
store = s3dlio.PyCheckpointStore("s3://bucket/checkpoints", "round_robin", None)
writer = store.writer(world_size=4, rank=0)
manifest_key = writer.finalize_distributed_checkpoint(
    step=100, epoch=10, framework="pytorch", shard_metas=shard_keys
)
```

### Test Coverage
- **Rust Tests**: Integration and advanced checkpoint operations (7 tests)
- **Python Tests**: Basic operations and framework integration (7 tests)  
- **Framework Support**: PyTorch, JAX, TensorFlow serialization validation
- **Multi-Backend**: All storage strategies tested across backends

ðŸ“– **[Checkpoint Documentation](Checkpoint_Features.md)** - Complete API reference, examples, and best practices

---

## What's new in v0.5.2 - Multi-Backend CLI Support

Added comprehensive multi-backend CLI support with a new unified `ls` command that works across all storage backends. This release provides a seamless CLI experience while maintaining full backward compatibility with existing S3-specific commands.

### ðŸŽ¯ Key Features

- **Unified CLI Interface**: New generic `ls` command supporting all storage backends via URI schemes
- **Enhanced URI Scheme Support**: Full support for `file://`, `direct://`, `s3://`, and `az://` schemes
- **Automatic Backend Routing**: CLI automatically selects the appropriate backend based on URI scheme
- **Backward Compatibility**: All existing S3-specific commands continue to work unchanged
- **Code Quality Improvements**: Added centralized constants module for better maintainability

### ðŸ”§ Technical Enhancements

- **src/bin/cli.rs**: Added `GenericList` command and `generic_list_cmd()` async function
- **src/file_store_direct.rs**: Updated URI validation to accept both `file://` and `direct://` schemes
- **src/constants.rs**: New centralized constants module replacing hardcoded values
- **Enhanced Error Handling**: Improved error messages for unsupported URI schemes

### ðŸ“š Documentation

- **docs/BACKEND_IMPROVEMENT_PLAN.md**: Added comprehensive backend improvement planning documentation
- **Migration Path**: Clear upgrade path from S3-specific to generic commands

### ðŸ§ª Testing & Validation

- All existing tests continue to pass
- New URI scheme validation ensures robust scheme handling
- Release build validation confirms production readiness

This release represents a significant step toward true multi-backend transparency, where users can seamlessly work with different storage systems using the same CLI interface.

---

## Version 0.5.3 - Enhanced Async Pool DataLoader with Backward Compatibility

Added a revolutionary async pooling DataLoader that provides dynamic batch formation with out-of-order completion, eliminating head-of-line blocking for high-throughput ML workloads. The key innovation is **complete backward compatibility** - existing code continues to work unchanged with traditional sequential loading, while new code can opt into async pooling for significant performance improvements.

### Key Features
- **Dynamic Batch Formation**: Batches form from completed requests rather than waiting for specific items in order
- **Out-of-Order Completion**: Eliminates head-of-line blocking where slow requests delay entire batches  
- **Complete Backward Compatibility**: Existing DataLoader code continues working unchanged
- **Multi-Backend Support**: Works seamlessly across all 4 storage backends (file://, direct://, s3://, az://)
- **Configurable Performance**: Conservative, Balanced, and Aggressive presets for different workload requirements

### Quick Start
```rust
// Traditional approach (unchanged)
let loader = DataLoader::new(dataset, LoaderOptions::default());

// New unified approach with async pooling
let loader = UnifiedDataLoader::new(dataset, 
    LoaderOptions::default().async_pool_loading()
);
```

ðŸ“– **[Technical Documentation](Enhanced_Async_Pool_DataLoader.md)** - Complete API reference, architecture details, and performance tuning guide

ðŸš€ **[Demo Application](../examples/async_pool_dataloader_demo.rs)** - Comprehensive demonstration with 4 different scenarios

---

## Version 0.5.1 - O_DIRECT File I/O Support

Added comprehensive O_DIRECT file I/O support for high-performance AI/ML workloads that need to bypass the OS page cache. This implementation provides graceful fallback to regular I/O when O_DIRECT is not supported, automatic system page size detection, and configurable alignment options.

### New Features
- `ConfigurableFileSystemObjectStore` with O_DIRECT support
- Automatic system page size detection via `libc::sysconf`
- Graceful fallback to regular I/O when O_DIRECT is unavailable
- Factory functions: `direct_io_store_for_uri()` and `high_performance_store_for_uri()`
- Comprehensive configuration options for alignment, sync writes, and minimum I/O sizes

### New Tests
- tests/test_direct_io.rs - 10 comprehensive tests covering O_DIRECT functionality, fallback mechanisms, and cross-platform compatibility

### New Documentation
- docs/O_DIRECT_Implementation.md - Complete implementation guide and usage examples

---

## Version 0.5.0 - Rust only - File and Azure Blob

Enhanced the support of the two new storage backends, Posix file and Azure blob. These remain "Rust Library" only enhancements, however, exposing the minimal changes to the python library should be straightforward. This represents a significant enhancement of this project's capabilities.

ðŸ“– **[Complete Documentation](Vers_5-0_Backend-Parity.md)** - Full details on these enhancements

---

## Version 0.4.6 - Rust only - File Phase1

Added new bindings and backend to support Posix File storage. As of this initial phase1 release, there are only minimal interfaces. Until this becomes fully flushed out, will remain as Rust only changes.

### New Tests
- test_file_store.rs - 9 file I/O tests

---

## Version 0.4.5 - Rust only - Azure Phase1

Added new bindings and backend to support Azure blob storage. As of this initial phase1 release, there are only minimal interfaces to Azure blob. Until this becomes fully flushed out, will remain as Rust only changes.

### New Tests
- azure_blob_multi.rs
- azure_blob_sequence.rs
- azure_blob_smoke.rs

---

## Version 0.4.4 - Multi-Part Upload with Zero-Copy

Added a multi-part uploader with zero buffer copies between Python and Rust. This works by providing a function that Rust executes and allocates the space for. Python can then fill the memory region and give it back to Rust. This all occurs zero-copy with Rust managing lifetimes. The memory region can be streamed to the S3 back-end, including using multi-part upload.

### New Tests
- python/tests/multi-part_smoke.py
- python/tests/test_multipart_writer.py
- tests/test_multipart.rs

### New Documentation
- MultiPart_README.md - Complete guide to multi-part features

---

## Version 0.4.3 - Full PyTorch DataLoader Compatibility

After many promises of a fully functional and compatible PyTorch data loader, version 0.4.3 provides functional compatibility with the aws s3torchconnector library. Also updated the Dockerfile to enable building containers again.

### New Tests
- python/tests/aws-s3dlio_compare_suite.py
- python/tests/compare_aws_s3dlio_loaders.py
- python/tests/jax_tf_vers_4-3_demo.py
- python/tests/torch_vers_4-3_demo.py

### New Documentation
- DataLoader_README.md - Complete guide to data loader features

---

## Version 0.4.2 - Regular Expression and Recursive Operations

Added regular expression parsing to most commands including list, delete, get and download. Added new recursive options "-r" or "--recursive" that makes operations work recursively. If the target command ends with a trailing slash "/" character, internally append a regex wildcard ".*". Also added "create-bucket" and "delete-bucket" operators as standalone commands.

---

## Version 0.4.1 - Python Data Loader Bindings

Added several python bindings to the "data-loader" interface, for use within PyTorch, TensorFlow and Jax AI/ML workflows. This feature is deemed fully functional via the Python library interfaces. Basic data-loader tests via python are available in python/tests including tf_smoke.py, jax_smoke.py and pytorch_smoke.py.

---

## Version 0.4.0 - High-Level Data Loader Interface

Added a high-level "data-loader" interface to enable use within AI/ML workflows. The data loader provides basic functionality primarily through the Python library interface where it is expected to receive most use.

---

## Version 0.3.x - I/O Tracing and Async Improvements

Added new I/O tracing capabilities using the same file format as the MinIO warp tool, enabling capture in a semi-standard way. With an I/O trace file, this enables recording and playback of S3 I/O operations. Also improved and updated the Rust-Python binding async I/O library for higher multi-threading with lower overhead and increased compatibility.

---

## Version 0.3.0 - Enhanced Data Generation and AI/ML Objects

Since the prior project "dlio_s3_rust" was archived and all future work moved to this project, several enhancements were added:
- Enhanced data generation with settable dedupe and compression ratios
- Two new cli commands: upload and download
- New "stat" command for both CLI and Python library
- AI/ML object creation options supporting 4 specific data/object types: TensorRecord, HDF5, NPZ, and Raw

---

## What's new in v0.3.3 branch: feature/data-loader (Stage 1 â€“ Core DataLoader)# Whatâ€™s new in v0.3.3 branch: feature/data-loader (Stage 1 â€“ Core DataLoader)

In this first incremental release of our high-level data-loading API, weâ€™ve added:

### 1. Core abstractions (`src/data_loader/`)

- **`Dataset` trait** (`dataset.rs`)  
  - `len() -> Option<usize>`, `get(index)`, and `as_stream()`  
  - Supports both map-style (random access) and iterable datasets  
- **`LoaderOptions`** (`options.rs`)  
  - Two tuning knobs: `batch_size` and `drop_last`  
- **`DataLoader`** (`dataloader.rs`)  
  - `.stream()` returns an `async_stream` of `Vec<Item>` batches  
  - Automatically handles:
    - Map-style datasets with known/unknown length  
    - Iterable-only streams  
    - Final incomplete batch drop if requested  

### 2. Crate-root exports (`src/lib.rs`)

- Publicly re-exported:
  - `Dataset`, `DatasetError`, `DynStream`
  - `LoaderOptions`
  - `DataLoader`

### 3. Baseline tests (`tests/test_dataloader.rs`)

- Map-style dataset batching  
- Drop-last semantics  
- Iterable-only datasets  
- Unknown-length datasets  
- Empty datasets  

### 4. Divergences / next steps

| Planned in Stage 1                                                | Status                                                |
|------------------------------------------------------------------|-------------------------------------------------------|
| First-class `Dataset` impls for RAW, NPZ, HDF5, TFRecord         | **Pending** (will add in Stage 2)                     |
| Python `pyo3` wrappers (`dataset.py` / `dataloader.py`)          | **Pending** (next release)                            |
| Smoke tests against a live MinIO bucket                          | Replaced by in-memory unit tests for fast iteration   |

Everything else in our Stage 1 blueprint is here and passing CIâ€”so you now have a minimal, idiomatic Rust API for batching S3-backed data, ready to build on in Stage 2 (shuffling, parallel fetch, auto-tuning).
