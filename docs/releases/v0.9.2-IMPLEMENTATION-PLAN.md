# v0.9.2 Implementation Plan - Universal Performance & Cleanup

## Overview
Version 0.9.2 will focus on:
1. **Stage 3**: Backend-agnostic concurrent range engine (performance)
2. **CLI Cleanup**: Remove S3-specific duplication, unify on universal APIs
3. **Code Path Consolidation**: Reduce S3-specific vs universal backend divergence

## Current State Analysis

### Issue 1: Range Engine Only for S3
**Status**: RangeEngine exists (`src/range_engine.rs`) but is **UNUSED**!

**Evidence**:
```bash
$ grep -r "RangeEngine::new" src/*.rs
# Only appears in range_engine.rs tests, nowhere else!
```

**Current Behavior**:
- S3: Uses `sharded_client.rs::get_ranges_concurrent()` 
- Other backends: Simple sequential reads
- RangeEngine exists but isn't integrated anywhere

**Root Cause**: RangeEngine was built but never integrated into actual download paths

### Issue 2: CLI Command Duplication
**Commands**:
1. `list` (S3-only, deprecated with warning, marked for removal in v0.9.0 but still present)
2. `ls` (Universal, works with all backends)

**Handler Analysis**:
```rust
// S3-only (deprecated)
Command::List { s3_path, recursive } => {
    check_aws_credentials()?;
    eprintln!("WARNING: The 'list' command is deprecated...");
    s3dlio::s3_utils::list_objects(bucket, key, recursive)?;
}

// Universal (current)
Command::GenericList { uri, recursive, pattern } => {
    let store = store_for_uri_with_logger(uri, logger)?;
    let keys = store.list(uri, recursive).await?;
}
```

**Decision**: Remove deprecated `list` command entirely

### Issue 3: S3 Utils Duplication
**Files**:
- `src/s3_utils.rs` - S3-specific functions
- `src/object_store.rs` - Universal ObjectStore trait

**S3-specific functions still used**:
```bash
list_objects()       # Used by deprecated CLI command
get_object_uri()     # Used in some places
put_objects_*()      # Used for bulk operations
```

**Goal**: Migrate all usage to universal ObjectStore trait

---

## Implementation Plan

### Phase 1: Range Engine Integration (Stage 3) ðŸŽ¯
**Goal**: Enable concurrent range downloads for ALL backends

#### Step 1.1: Audit Current Get Paths
**Files to check**:
- `src/object_store.rs` - S3ObjectStore::get()
- `src/file_store.rs` - FileSystemObjectStore::get()
- `src/file_store_direct.rs` - ConfigurableFileSystemObjectStore::get()
- `src/azure_client.rs` - Azure get implementation
- `src/gcs_client.rs` - GCS get implementation

**Questions**:
1. Do any backends currently use concurrent ranges?
2. Where does the optimization threshold decision happen?
3. How is RangeEngine supposed to be used?

#### Step 1.2: Extract Generic Range Logic
**Current**: RangeEngine is S3-agnostic but unused
**Plan**:
1. Review `RangeEngine::download_with_ranges()`
2. Identify S3-specific assumptions (if any)
3. Make it work with generic async `get_range()` function

#### Step 1.3: Integrate RangeEngine per Backend

**FileSystemObjectStore (file://):**
```rust
async fn get(&self, uri: &str) -> Result<Bytes> {
    let size = self.stat(uri).await?.size;
    
    // Use range engine for large files
    if size > self.range_threshold {
        return self.get_with_ranges(uri, size).await;
    }
    
    // Simple read for small files
    tokio::fs::read(path).await?.into()
}

async fn get_with_ranges(&self, uri: &str, size: u64) -> Result<Bytes> {
    let engine = RangeEngine::new(self.config);
    engine.download(uri, size, |uri, offset, length| {
        self.get_range(uri, offset, length)
    }).await
}
```

**Similar pattern for**:
- DirectIO backend
- Azure backend
- GCS backend

#### Step 1.4: Configuration
**Add to each backend**:
- `range_threshold`: File size to trigger concurrent ranges (default: 4MB)
- `range_chunk_size`: Size of each range chunk (default: 64MB)
- `max_concurrent_ranges`: Parallel range downloads (default: 32)

#### Step 1.5: Testing
- [ ] Unit tests per backend
- [ ] Integration tests with various file sizes
- [ ] Performance benchmarks (before/after)
- [ ] Verify correctness with checksums

---

### Phase 2: CLI Cleanup ðŸ§¹

#### Step 2.1: Remove Deprecated `list` Command
**File**: `src/bin/cli.rs`

**Actions**:
1. Remove `List` variant from `Command` enum (line ~145)
2. Remove handler `Command::List { ... }` (line ~460)
3. Update help text to only show `ls`

**Migration path**:
```bash
# OLD (remove)
s3-cli list s3://bucket/prefix/ -r

# NEW (keep)
s3-cli ls s3://bucket/prefix/ -r
```

#### Step 2.2: Audit Other S3-Specific Commands
**Check if these can be unified**:
- `Stat` - Does it work with all backends? Should be universal
- Any other S3Path-specific commands?

**Pattern**: Replace `S3Path` type with generic `String uri` where possible

---

### Phase 3: Code Path Consolidation ðŸ”€

#### Step 3.1: Audit s3_utils.rs Usage
**Find all call sites**:
```bash
grep -rn "s3_utils::" src/
```

**For each function**:
1. Check if universal equivalent exists in ObjectStore
2. Migrate callers to use ObjectStore trait
3. Mark S3-specific function as deprecated or remove

#### Step 3.2: Python API Consolidation
**File**: `src/python_api/python_core_api.rs`

**Current**: Mix of S3-specific and universal calls
**Goal**: Use ObjectStore trait exclusively

**Example migration**:
```rust
// OLD
pub fn get_object(bucket: &str, key: &str) -> PyResult<PyBytesView> {
    let bytes = s3_utils::get_object(bucket, key)?;
    Ok(PyBytesView::new(bytes))
}

// NEW
pub fn get_object(uri: &str) -> PyResult<PyBytesView> {
    let store = store_for_uri(uri)?;
    let bytes = store.get(uri).await?;
    Ok(PyBytesView::new(bytes))
}
```

#### Step 3.3: Identify Legitimate S3-Only Functions
**Keep in s3_utils.rs** (genuinely S3-specific):
- Bucket management (create/delete buckets)
- S3-specific configuration
- S3-specific optimizations (if any)

**Everything else**: Move to ObjectStore trait or remove

---

## Detailed Step-by-Step

### Week 1: Range Engine (Days 1-3)

#### Day 1: Audit & Design
- [ ] Map all `get()` implementations across backends
- [ ] Understand current `RangeEngine` API
- [ ] Design integration pattern for each backend
- [ ] Create test plan

#### Day 2: File Backend Implementation
- [ ] Integrate RangeEngine with FileSystemObjectStore
- [ ] Add configuration options
- [ ] Write unit tests
- [ ] Benchmark performance

#### Day 3: Other Backends
- [ ] DirectIO backend integration
- [ ] Azure backend integration
- [ ] GCS backend integration
- [ ] Run full test suite

### Week 2: Cleanup & Polish (Days 4-5)

#### Day 4: CLI Cleanup
- [ ] Remove deprecated `list` command
- [ ] Audit all S3Path usage
- [ ] Unify on universal commands
- [ ] Update documentation

#### Day 5: Code Consolidation
- [ ] Audit s3_utils.rs usage
- [ ] Migrate to ObjectStore where possible
- [ ] Update Python API
- [ ] Remove dead code

---

## Testing Strategy

### Performance Testing
**Benchmark each backend**:
```bash
# Before/after comparison
./scripts/benchmark_backend.sh --backend file --size 1GB
./scripts/benchmark_backend.sh --backend s3 --size 1GB
./scripts/benchmark_backend.sh --backend azure --size 1GB
./scripts/benchmark_backend.sh --backend gcs --size 1GB
```

**Expected Results**:
- 30-50% throughput improvement for large files (>4MB)
- No regression for small files
- Consistent behavior across all backends

### Functional Testing
- [ ] All Rust tests pass
- [ ] All Python tests pass
- [ ] CLI commands work identically across backends
- [ ] Zero-copy still works
- [ ] Range requests still work

---

## Success Criteria

### Performance
- âœ… File backend: 30%+ throughput improvement for files >100MB
- âœ… Azure backend: 30%+ throughput improvement for files >100MB
- âœ… GCS backend: 30%+ throughput improvement for files >100MB
- âœ… DirectIO backend: 30%+ throughput improvement for files >100MB
- âœ… S3 backend: No regression

### Code Quality
- âœ… Removed deprecated `list` command
- âœ… S3-specific code paths reduced by 50%+
- âœ… All backends use ObjectStore trait
- âœ… Zero compiler warnings
- âœ… All tests pass

### User Experience
- âœ… CLI commands work identically across backends
- âœ… Performance improvement transparent to users
- âœ… No breaking API changes
- âœ… Documentation updated

---

## Questions to Resolve

### 1. RangeEngine API Design
How should backends call RangeEngine? Options:
- **A**: RangeEngine takes a closure: `engine.download(uri, size, |u, o, l| backend.get_range(u, o, l))`
- **B**: RangeEngine is generic over backend trait
- **C**: RangeEngine is instantiated per backend type

**Recommendation**: Option A (closure) - most flexible

### 2. Configuration
Where should range engine config live?
- **A**: Per-backend in ObjectStore struct
- **B**: Global configuration
- **C**: Environment variables

**Recommendation**: Option A (per-backend) - most flexible

### 3. Threshold Tuning
What should the default threshold be?
- Current: 4MB seems reasonable
- Could be different per backend (network vs local)
- Should be configurable

**Recommendation**: Start with 4MB default, make configurable

---

## Version 0.9.2 Release

**Release Type**: Performance enhancement (non-breaking)

**Changelog**:
```markdown
## v0.9.2 - Backend Performance Unification

### Performance
- **Stage 3 Complete**: All backends now use concurrent range downloads for large files
  - File backend: 30-50% throughput improvement
  - DirectIO backend: 30-50% throughput improvement  
  - Azure backend: 30-50% throughput improvement
  - GCS backend: 30-50% throughput improvement
- Automatic optimization for files >4MB (configurable)
- Zero API changes, fully backward compatible

### CLI
- Removed deprecated `list` command (use `ls` for all backends)
- Unified command interface across all storage backends
- Consistent behavior for s3://, az://, gs://, file://, direct://

### Code Quality
- Reduced S3-specific code paths by 50%+
- Consolidated on universal ObjectStore trait
- Removed code duplication
- Zero compiler warnings

### Testing
- All Rust tests pass (91/91)
- All Python tests pass (27/27)
- Performance benchmarks validate improvements
```

---

## Next Steps

1. **Start with audit**: Map all get() implementations
2. **Prototype**: Integrate RangeEngine with File backend first
3. **Validate**: Measure performance improvement
4. **Iterate**: Apply to other backends
5. **Cleanup**: Remove deprecated commands
6. **Test**: Comprehensive validation
7. **Document**: Update all docs
8. **Release**: v0.9.2

**ETA**: 5-7 days for complete implementation
