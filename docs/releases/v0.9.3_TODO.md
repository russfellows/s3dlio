# s3dlio Outstanding Work Items

**Date**: October 9, 2025  
**Current Version**: v0.9.2  
**Status**: Released with CancellationToken & Configuration Hierarchy

---

## Overview

This document consolidates all outstanding work items from recent planning documents (v0.9.0+). Items are organized by priority and completion status.

---

## ‚úÖ Recently Completed (v0.9.2)

### Phase 1-3: Generic RangeEngine Implementation
- ‚úÖ Created `src/range_engine_generic.rs` (492 lines, 4 tests)
- ‚úÖ Universal stream-based architecture
- ‚úÖ Integrated into FileSystemObjectStore
- ‚úÖ Integrated into DirectIOObjectStore (ConfigurableFileSystemObjectStore)
- ‚úÖ All tests passing (16/16)
- ‚úÖ Constants consolidated in `src/constants.rs`

### Phase 4: CancellationToken Infrastructure
- ‚úÖ Graceful shutdown for all DataLoader components
- ‚úÖ Updated LoaderOptions, spawn_prefetch(), DataLoader, AsyncPoolDataLoader
- ‚úÖ 9 comprehensive cancellation tests (all passing)
- ‚úÖ Documentation: `CANCELLATION-TOKEN-IMPLEMENTATION.md`

### Phase 5: Configuration Hierarchy Rationalization
- ‚úÖ Three-level design documented (LoaderOptions ‚Üí PoolConfig ‚Üí RangeEngineConfig)
- ‚úÖ PyTorch-aligned concepts
- ‚úÖ Documentation: `CONFIGURATION-HIERARCHY.md`
- ‚úÖ Updated API docs to v0.9.2
- ‚úÖ PoolConfig::from_loader_options() convenience constructor

### Test Fixes
- ‚úÖ Fixed all v0.9.2 test compilation errors (22+ Bytes comparisons, etc.)
- ‚úÖ 113/114 Rust tests passing (99.1%)
- ‚úÖ Test summary document created
- ‚úÖ README badges updated

---

## üî• High Priority Outstanding Items

### 1. Complete RangeEngine Backend Integration

**Status**: Partially complete (File + DirectIO done)

**Remaining Backends**:

#### A. Azure Backend Integration ‚è≥
**Priority**: HIGH  
**Effort**: 2-3 hours  

**Work Items**:
- [ ] Integrate RangeEngine into `src/azure_store.rs`
- [ ] Add configuration for range engine thresholds
- [ ] Test with Azure Blob Storage
- [ ] Benchmark performance improvement (expect 30-50%)
- [ ] Update documentation

**Expected Outcome**:
- Concurrent range downloads for large Azure blobs
- Significant throughput improvement for files > 4MB
- Zero API changes (transparent optimization)

---

#### B. GCS Backend Integration ‚è≥
**Priority**: HIGH  
**Effort**: 2-3 hours  

**Work Items**:
- [ ] Integrate RangeEngine into `src/gcs_store.rs`
- [ ] Add configuration for range engine thresholds
- [ ] Test with Google Cloud Storage
- [ ] Benchmark performance improvement (expect 30-50%)
- [ ] Update documentation

**Expected Outcome**:
- Concurrent range downloads for large GCS objects
- Significant throughput improvement for files > 4MB
- Zero API changes (transparent optimization)

---

#### C. S3 Backend Evaluation ‚è≥
**Priority**: MEDIUM  
**Effort**: 1-2 hours  

**Work Items**:
- [ ] Compare existing sharded_client approach vs RangeEngine
- [ ] Benchmark both implementations
- [ ] Decision: keep current or migrate to RangeEngine
- [ ] Document decision rationale

**Notes**:
- S3 backend already has optimized concurrent range downloads via `ShardedS3Clients`
- May not need RangeEngine integration
- Should measure to determine if RangeEngine provides additional benefit

---

### 2. GCS Backend Completion Testing

**Status**: Backend functional, needs comprehensive validation

**Priority**: HIGH  
**Effort**: Medium (1-2 days)

Per `POST-v0.9.1-TODO.md` and `GCS-TESTING-SUMMARY.md`, the GCS backend works but needs:

#### A. Python API Testing ‚è≥
**File**: `python/tests/test_gcs_api.py` (needs creation)

**Work Items**:
- [ ] Test basic operations (get, put, list, delete) with gs:// URIs
- [ ] Test zero-copy `get()` with GCS
- [ ] Test zero-copy `get_range()` with GCS
- [ ] Test `get_many()` with GCS
- [ ] Test PyTorch DataLoader integration with GCS
- [ ] Test TensorFlow Dataset integration with GCS
- [ ] Document any GCS-specific quirks

---

#### B. Performance Benchmarking ‚è≥
**Scripts**: Update `scripts/run_backend_comparison.sh`

**Work Items**:
- [ ] Add GCS to performance comparison benchmarks
- [ ] Measure upload throughput (target: 2.5+ GB/s)
- [ ] Measure download throughput (target: 5+ GB/s)
- [ ] Compare: S3 vs Azure vs GCS vs File vs DirectIO
- [ ] Test concurrent operations scaling
- [ ] Test large file handling (>10GB)
- [ ] Document performance characteristics

---

#### C. Error Handling Edge Cases ‚è≥
**File**: `tests/test_gcs_errors.rs` (needs creation)

**Work Items**:
- [ ] Test network failures during upload/download
- [ ] Test authentication token expiration
- [ ] Test invalid bucket/object names
- [ ] Test permission denied errors
- [ ] Test quota exceeded scenarios
- [ ] Verify error propagation to Python
- [ ] Verify error messages are clear

---

### 3. CLI Cleanup - Remove Deprecated Commands

**Priority**: MEDIUM  
**Effort**: 1-2 hours

**Work Items**:
- [ ] Remove deprecated `list` command (S3-specific)
- [ ] Keep only `ls` command (universal)
- [ ] Update any remaining deprecation warnings
- [ ] Update tests that use `list` command
- [ ] Verify all CLI tests pass

**Files to Modify**:
- CLI argument parsing
- Command dispatch logic
- Test files

**Benefits**:
- Cleaner CLI surface
- Less user confusion
- Reduced code maintenance

---

## üìã Medium Priority Items

### 4. Python DataLoader Zero-Copy Enhancement

**Priority**: MEDIUM  
**Effort**: Medium  
**Decision Needed**: v0.9.3 or v0.10.0?

**Background**: Dataset trait currently uses `Vec<u8>` (not zero-copy).

**Options**:
1. **Breaking Change (v0.10.0)**: Update Dataset trait to use `Bytes`
2. **Non-Breaking (v0.9.3)**: Add new trait methods returning `Bytes`
3. **Hybrid**: Deprecate `Vec<u8>` methods, add `Bytes` versions

**Impact**:
- DataLoader iterators would benefit from zero-copy
- Reduces memory allocations in training loops
- Better integration with framework zero-copy APIs

**Work Items** (if pursued):
- [ ] Decide on approach (breaking vs non-breaking)
- [ ] Update Dataset trait definition
- [ ] Update all Dataset implementations
- [ ] Update PyTorch/TensorFlow/JAX integrations
- [ ] Update Python bindings
- [ ] Update documentation and examples
- [ ] Migration guide if breaking change

---

### 5. Comprehensive Performance Benchmarking

**Priority**: MEDIUM  
**Effort**: 3-4 hours

**Work Items**:
- [ ] Add -v/-vv logging to benchmarks (using tracing framework)
- [ ] Measure actual performance improvements from RangeEngine
- [ ] Compare File vs DirectIO vs S3 vs Azure vs GCS
- [ ] Test various file sizes (1MB, 10MB, 100MB, 1GB, 10GB)
- [ ] Validate with checksums (ensure data integrity)
- [ ] Document results in performance guide
- [ ] Update README with benchmark results

**Target Metrics**:
- Read throughput: 5+ GB/s
- Write throughput: 2.5+ GB/s
- Range engine improvement: 30-50% for large files
- No regression for small files

---

## üìö Documentation Updates

### 6. Update Existing Documentation

**Priority**: MEDIUM  
**Effort**: Low (2-3 hours)

**Work Items**:
- [ ] Update `GCS_TODO.md` to reflect current status
- [ ] Mark completed items in `STAGE3-DEFERRAL.md`
- [ ] Update README with GCS examples
- [ ] Add GCS examples to API guides
- [ ] Document GCS authentication (ADC)
- [ ] Add GCS to backend comparison table
- [ ] Update performance documentation with RangeEngine results
- [ ] Create v0.9.3 planning document (if needed)

---

## üîÆ Future Enhancements (Lower Priority)

### 7. Additional GCS Features

#### A. List Buckets Support
**Priority**: LOW  
**Effort**: Medium  
**Status**: Document workaround

GCS doesn't expose project-level bucket listing easily.

**Options**:
1. Check if gcloud-storage SDK supports list_buckets() with project ID
2. Add --project flag to CLI if needed
3. **Recommended**: Document workaround (`gcloud storage buckets list`)

---

#### B. Resumable Upload Implementation
**Priority**: LOW  
**Effort**: High  

GCS supports resumable uploads for large files.

**Current State**:
- GcsBufferedWriter exists
- May not use resumable upload API
- Would benefit large uploads (>100MB)

**Work Items** (if pursued):
- [ ] Check if gcloud-storage SDK exposes resumable upload APIs
- [ ] Implement session token management
- [ ] Add retry logic with resume capability
- [ ] Test interruption/resume scenarios

---

## üéØ Recommended Sprint Plan

### Sprint 1: Complete RangeEngine Integration (3-5 days)

**Week 1 (October 9-11, 2025)**:
- Day 1: Azure backend RangeEngine integration
- Day 2: GCS backend RangeEngine integration
- Day 3: S3 backend evaluation and decision
- Day 4-5: Testing, benchmarking, documentation

**Deliverables**:
- All backends have concurrent range download capability (where beneficial)
- Performance benchmarks showing improvements
- Updated documentation

---

### Sprint 2: GCS Validation (1-2 days)

**Week 2 (October 14-15, 2025)**:
- Day 1: Python API testing with GCS
- Day 2: Performance benchmarking and error handling
- Ongoing: Update documentation

**Deliverables**:
- Comprehensive GCS test coverage
- Performance validation
- Production-ready GCS backend

---

### Sprint 3: Polish & Cleanup (1-2 days)

**Week 2 (October 16-17, 2025)**:
- CLI cleanup (remove deprecated commands)
- Comprehensive documentation updates
- Performance benchmarking across all backends
- Decision on Python DataLoader zero-copy (v0.9.3 vs v0.10.0)

**Deliverables**:
- Clean CLI surface
- Complete documentation
- Performance benchmark report
- v0.9.3 or v0.10.0 planning document

---

## ‚ùì Open Questions

### 1. Version Number Strategy
**Question**: Should Python DataLoader zero-copy be v0.9.3 or v0.10.0?

**Options**:
- **v0.9.3**: Non-breaking (add new Bytes methods, deprecate Vec<u8>)
- **v0.10.0**: Breaking change (update Dataset trait to use Bytes)

**Decision Factors**:
- User impact: How many users have custom Dataset implementations?
- Timeline: When do we want to make breaking changes?
- Migration effort: How complex is the upgrade path?

---

### 2. GCS Priority
**Question**: How critical is comprehensive GCS validation?

**Options**:
- **High Priority**: If GCS is needed for production deployments
- **Medium Priority**: If GCS is exploratory/experimental
- **Low Priority**: If focus is on S3/Azure

**Recommendation**: Clarify with user based on deployment needs.

---

### 3. S3 RangeEngine Migration
**Question**: Should S3 backend migrate to RangeEngine or keep sharded_client?

**Options**:
- **Keep Current**: If benchmarks show no improvement
- **Migrate**: If RangeEngine shows measurable benefit
- **Support Both**: Allow runtime selection

**Decision**: Requires benchmarking to measure.

---

## üìä Current Status Summary

### Completed Features (v0.9.2)
- ‚úÖ Generic RangeEngine architecture
- ‚úÖ File backend integration
- ‚úÖ DirectIO backend integration
- ‚úÖ CancellationToken infrastructure
- ‚úÖ Configuration hierarchy rationalization
- ‚úÖ Comprehensive testing (122/130 tests passing)

### In Progress
- üîÑ Azure backend RangeEngine integration
- üîÑ GCS backend RangeEngine integration
- üîÑ Comprehensive GCS testing

### Planned
- ‚è≥ CLI cleanup
- ‚è≥ Python DataLoader zero-copy
- ‚è≥ Comprehensive benchmarking
- ‚è≥ Documentation updates

---

## üìù Notes

- All work items reference recent planning documents (October 2025)
- Items from pre-v0.9.0 documentation excluded as requested
- Priority levels reflect user feedback and technical dependencies
- Sprint plan assumes full-time development (adjust for actual capacity)
- Performance targets based on existing benchmarks and infrastructure

---

**Last Updated**: October 9, 2025  
**Next Review**: After Sprint 1 completion  
**Related Documents**:
- `v0.9.2-NEXT-STEPS.md`
- `POST-v0.9.1-TODO.md`
- `v0.9.2-PROGRESS.md`
- `v0.9.2-SESSION-SUMMARY.md`
- `CONFIGURATION-HIERARCHY.md`
- `CANCELLATION-TOKEN-IMPLEMENTATION.md`
