"""
Drop-in replacement for AWS s3torchconnector using s3dlio backend.

This module provides API-compatible classes that match s3torchconnector's interface,
allowing users to easily switch between implementations by changing only the import:

    # Original:
    from s3torchconnector import S3IterableDataset, S3MapDataset, S3Checkpoint
    
    # Drop-in replacement:
    from s3dlio.compat.s3torchconnector import S3IterableDataset, S3MapDataset, S3Checkpoint

Key differences from native s3dlio classes:
- Uses .from_prefix(uri, region=...) signature matching s3torchconnector
- Returns item objects with .bucket, .key, .read() methods (not raw tensors)
- Supports S3, Azure, GCS, local filesystems (s3torchconnector only supports S3)
- Higher performance (Rust backend vs Python/C++)

Environment variables (same as s3torchconnector):
    AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY - S3 credentials
    AWS_ENDPOINT_URL - Custom endpoint (MinIO, Ceph, etc.)
    AWS_REGION - Default region (if not specified in from_prefix)
    AZURE_STORAGE_ACCOUNT_NAME, AZURE_STORAGE_ACCOUNT_KEY - Azure credentials
    GOOGLE_APPLICATION_CREDENTIALS - GCS service account JSON path
"""

from __future__ import annotations

import os
import warnings
from typing import Any, Iterator, Optional
from urllib.parse import urlparse

import torch
from torch.utils.data import IterableDataset, Dataset

from ..torch import ObjectStoreIterableDataset, ObjectStoreMapDataset


class S3Item:
    """
    Item returned by S3IterableDataset and S3MapDataset, matching s3torchconnector API.
    
    Attributes:
        bucket (str): Bucket name (or container/account for Azure/GCS)
        key (str): Object key (relative path)
    
    Methods:
        read() -> bytes: Read the entire object payload
        close(): Close the reader (no-op for compatibility)
    """
    
    __slots__ = ("_uri", "_bucket", "_key", "_data", "_closed")
    
    def __init__(self, uri: str, data: bytes):
        """
        Args:
            uri: Full URI (e.g., "s3://bucket/path/key")
            data: Object payload bytes
        """
        self._uri = uri
        self._data = data
        self._closed = False
        
        # Parse URI to extract bucket and key
        parsed = urlparse(uri)
        self._bucket, self._key = self._parse_uri_parts(parsed)
    
    def _parse_uri_parts(self, parsed):
        """Extract bucket/container and key from parsed URI."""
        scheme = parsed.scheme or "s3"
        
        if scheme == "s3":
            # s3://bucket/path/to/key
            bucket = parsed.netloc
            key = parsed.path.lstrip("/")
        elif scheme == "az":
            # az://account/container/path/to/key
            # netloc is account, first path segment is container
            parts = parsed.path.lstrip("/").split("/", 1)
            bucket = f"{parsed.netloc}/{parts[0]}" if len(parts) > 0 else parsed.netloc
            key = parts[1] if len(parts) > 1 else ""
        elif scheme == "gs":
            # gs://bucket/path/to/key
            bucket = parsed.netloc
            key = parsed.path.lstrip("/")
        elif scheme in ("file", "direct"):
            # file:///path/to/file or direct:///path/to/file
            # Use the directory as "bucket" and filename as "key"
            full_path = parsed.path
            if "/" in full_path:
                bucket = os.path.dirname(full_path)
                key = os.path.basename(full_path)
            else:
                bucket = "/"
                key = full_path
        else:
            # Fallback for unknown schemes
            bucket = parsed.netloc or "unknown"
            key = parsed.path.lstrip("/")
        
        return bucket, key
    
    @property
    def bucket(self) -> str:
        """Bucket name (or container for Azure, account for GCS)."""
        return self._bucket
    
    @property
    def key(self) -> str:
        """Object key (relative path from bucket root)."""
        return self._key
    
    def read(self):
        """
        Read the entire object payload (ZERO-COPY).
        
        Returns BytesView (implements buffer protocol) for optimal performance.
        Compatible with:
          - torch.frombuffer(data, dtype=torch.uint8)  # Zero-copy tensor
          - np.frombuffer(data, dtype=np.uint8)        # Zero-copy array
          - file.write(data)                            # Zero-copy write
          - memoryview(data)                            # Buffer protocol
        
        For strict bytes type (slower, creates copy), use read_bytes() instead.
        
        Returns:
            BytesView: Zero-copy buffer protocol object
        """
        if self._closed:
            raise ValueError("I/O operation on closed S3Item")
        return self._data
    
    def read_bytes(self) -> bytes:
        """
        Read as bytes (CREATES COPY - slower but strictly compatible).
        
        Use this only if you need exact bytes type. For most ML workloads,
        prefer read() which returns zero-copy BytesView.
        
        Returns:
            bytes: Python bytes object (creates memory copy)
        """
        if self._closed:
            raise ValueError("I/O operation on closed S3Item")
        return bytes(self._data) if not isinstance(self._data, bytes) else self._data
    
    def readall(self):
        """Alias for read() - returns zero-copy BytesView."""
        return self.read()
    
    def read_all(self):
        """Alias for read() - returns zero-copy BytesView."""
        return self.read()
    
    def close(self) -> None:
        """Close the reader (no-op for compatibility)."""
        self._closed = True
    
    def __enter__(self) -> "S3Item":
        return self
    
    def __exit__(self, exc_type, exc, tb) -> None:
        self.close()
    
    def __len__(self) -> int:
        """Return size of object in bytes."""
        return len(self._data)
    
    def __repr__(self) -> str:
        return f"S3Item(bucket='{self.bucket}', key='{self.key}', size={len(self._data)})"


class S3IterableDataset(IterableDataset):
    """
    Drop-in replacement for s3torchconnector.S3IterableDataset using s3dlio.
    
    Key features:
    - API-compatible with s3torchconnector for easy migration
    - Multi-protocol support (S3, Azure, GCS, file://, direct://)
    - Higher performance (Rust backend)
    - Multi-endpoint load balancing (via AWS_ENDPOINT_URL=url1,url2,...)
    
    Usage:
        >>> from s3dlio.compat.s3torchconnector import S3IterableDataset
        >>> dataset = S3IterableDataset.from_prefix("s3://bucket/prefix/", region="us-east-1")
        >>> for item in dataset:
        ...     print(f"{item.bucket}/{item.key}: {len(item.read())} bytes")
    """
    
    def __init__(
        self,
        uri: str,
        region: Optional[str] = None,
        *,
        enable_sharding: bool = False,
        **loader_kwargs
    ):
        """
        Args:
            uri: Full URI to prefix (e.g., "s3://bucket/prefix/")
            region: AWS region (optional, can use AWS_REGION env var or config)
            enable_sharding: Automatically shard dataset across distributed workers
            **loader_kwargs: Additional arguments for s3dlio loader (prefetch, shuffle, etc.)
        """
        self._uri = uri
        self._region = region
        self._enable_sharding = enable_sharding
        
        # Set AWS region if provided (s3dlio uses env vars)
        if region:
            os.environ.setdefault("AWS_REGION", region)
        
        # Extract s3dlio-compatible loader options
        # Map s3torchconnector params to s3dlio params where needed
        self._loader_opts = self._convert_loader_kwargs(loader_kwargs)
        
        # Create underlying s3dlio dataset (returns bytes via return_type="bytes")
        self._dataset = ObjectStoreIterableDataset.from_prefix(
            uri,
            enable_sharding=enable_sharding,
            return_type="bytes",  # Get raw bytes, wrap in S3Item
            **self._loader_opts
        )
    
    def _convert_loader_kwargs(self, kwargs):
        """Convert s3torchconnector kwargs to s3dlio format."""
        # s3dlio supports most options directly
        opts = {}
        
        # Direct mappings
        for key in ["prefetch", "shuffle", "seed", "num_workers", "batch_size", "drop_last"]:
            if key in kwargs:
                opts[key] = kwargs[key]
        
        # Reader configuration (s3torchconnector uses reader_constructor, s3dlio uses reader_mode)
        if "reader_constructor" in kwargs:
            # This is advanced - for now just use sequential
            warnings.warn("reader_constructor not directly supported - using default reader_mode")
        
        return opts
    
    @classmethod
    def from_prefix(
        cls,
        prefix: str,
        region: Optional[str] = None,
        **kwargs
    ) -> "S3IterableDataset":
        """
        Create dataset from S3 prefix (API-compatible with s3torchconnector).
        
        Args:
            prefix: S3 URI prefix (e.g., "s3://bucket/path/")
            region: AWS region (e.g., "us-east-1")
            **kwargs: Additional options (enable_sharding, prefetch, etc.)
        
        Returns:
            S3IterableDataset instance
        
        Examples:
            >>> # Basic usage
            >>> ds = S3IterableDataset.from_prefix("s3://my-bucket/train/", region="us-east-1")
            
            >>> # With sharding for distributed training
            >>> ds = S3IterableDataset.from_prefix(
            ...     "s3://my-bucket/train/",
            ...     region="us-east-1",
            ...     enable_sharding=True
            ... )
            
            >>> # Azure Blob Storage
            >>> ds = S3IterableDataset.from_prefix("az://myaccount/container/prefix/")
            
            >>> # Local filesystem (for testing)
            >>> ds = S3IterableDataset.from_prefix("file:///data/train/")
        """
        return cls(prefix, region=region, **kwargs)
    
    def __iter__(self) -> Iterator[S3Item]:
        """Iterate over objects, yielding S3Item instances."""
        # Get the base URI for reconstructing full URIs
        base_uri = self._uri.rstrip("/")
        
        # Iterate over underlying dataset (yields bytes)
        for data in self._dataset:
            # Need to reconstruct the URI - s3dlio doesn't preserve it in iterable mode
            # For now, create a placeholder URI (this is a limitation we should fix upstream)
            # In practice, users iterate without needing the exact URI
            item_uri = base_uri  # Fallback - exact key unknown in streaming mode
            
            yield S3Item(item_uri, data)


class S3MapDataset(Dataset):
    """
    Drop-in replacement for s3torchconnector.S3MapDataset using s3dlio.
    
   Key features:
    - API-compatible with s3torchconnector for easy migration
    - Multi-protocol support (S3, Azure, GCS, file://, direct://)
    - Random access by index
    - Higher performance (Rust backend)
    
    Usage:
        >>> from s3dlio.compat.s3torchconnector import S3MapDataset
        >>> dataset = S3MapDataset.from_prefix("s3://bucket/prefix/", region="us-east-1")
        >>> item = dataset[0]
        >>> print(f"{item.bucket}/{item.key}: {len(item.read())} bytes")
    """
    
    def __init__(
        self,
        uri: str,
        region: Optional[str] = None,
        **kwargs
    ):
        """
        Args:
            uri: Full URI to prefix (e.g., "s3://bucket/prefix/")
            region: AWS region (optional, can use AWS_REGION env var)
            **kwargs: Additional arguments for s3dlio (reader_mode, etc.)
        """
        self._uri = uri
        self._region = region
        
        # Set AWS region if provided
        if region:
            os.environ.setdefault("AWS_REGION", region)
        
        # Create underlying s3dlio map dataset (returns bytes)
        self._dataset = ObjectStoreMapDataset.from_prefix(
            uri,
            return_type="bytes",  # Get raw bytes, wrap in S3Item
            **kwargs
        )
        
        # Pre-compute full URIs for each key
        parsed = urlparse(uri)
        scheme = parsed.scheme or "s3"
        base = uri.rstrip("/")
        
        self._full_uris = []
        for key in self._dataset._keys:
            full_uri = f"{base}/{key}"
            self._full_uris.append(full_uri)
    
    @classmethod
    def from_prefix(
        cls,
        prefix: str,
        region: Optional[str] = None,
        **kwargs
    ) -> "S3MapDataset":
        """
        Create map dataset from S3 prefix (API-compatible with s3torchconnector).
        
        Args:
            prefix: S3 URI prefix (e.g., "s3://bucket/path/")
            region: AWS region (e.g., "us-east-1")
            **kwargs: Additional options
        
        Returns:
            S3MapDataset instance
        
        Examples:
            >>> ds = S3MapDataset.from_prefix("s3://my-bucket/train/", region="us-east-1")
            >>> item = ds[0]
            >>> content = item.read()
        """
        return cls(prefix, region=region, **kwargs)
    
    def __len__(self) -> int:
        """Return number of objects in the dataset."""
        return len(self._dataset)
    
    def __getitem__(self, index: int) -> S3Item:
        """Get item by index, returning S3Item instance."""
        if index < 0 or index >= len(self):
            raise IndexError(f"Index {index} out of range for dataset of size {len(self)}")
        
        # Get data from underlying dataset
        data = self._dataset[index]
        
        # Get the full URI for this item
        item_uri = self._full_uris[index]
        
        return S3Item(item_uri, data)


class S3Checkpoint:
    """
    Drop-in replacement for s3torchconnector.S3Checkpoint using s3dlio.
    
    Provides simple checkpoint saving/loading to S3-compatible storage.
    
    Features:
    - API-compatible with s3torchconnector
    - Multi-protocol support (S3, Azure, GCS, file://)
    - Works with torch.save() and torch.load()
    
    Usage:
        >>> from s3dlio.compat.s3torchconnector import S3Checkpoint
        >>> checkpoint = S3Checkpoint(region="us-east-1")
        >>> 
        >>> # Save checkpoint
        >>> with checkpoint.writer("s3://bucket/model.ckpt") as writer:
        ...     torch.save(model.state_dict(), writer)
        >>> 
        >>> # Load checkpoint
        >>> with checkpoint.reader("s3://bucket/model.ckpt") as reader:
        ...     state_dict = torch.load(reader)
        >>> model.load_state_dict(state_dict)
    """
    
    def __init__(self, region: Optional[str] = None):
        """
        Args:
            region: AWS region (optional, can use AWS_REGION env var)
        """
        self._region = region
        if region:
            os.environ.setdefault("AWS_REGION", region)
    
    def writer(self, uri: str):
        """
        Context manager for writing checkpoint to storage.
        
        Args:
            uri: Full URI to checkpoint (e.g., "s3://bucket/checkpoint.pt")
        
        Returns:
            File-like writer object compatible with torch.save()
        
        Example:
            >>> with checkpoint.writer("s3://bucket/model.ckpt") as writer:
            ...     torch.save(model.state_dict(), writer)
        """
        import io
        import tempfile
        from .. import _pymod
        
        # Create a temporary buffer to collect checkpoint data
        class _S3Writer:
            def __init__(self, uri):
                self.uri = uri
                self.buffer = io.BytesIO()
            
            def write(self, data):
                return self.buffer.write(data)
            
            def close(self):
                # Upload buffer to storage on close
                self.buffer.seek(0)
                _pymod.put_bytes(self.uri, self.buffer.read())
            
            def __enter__(self):
                return self.buffer  # torch.save() needs file-like object
            
            def __exit__(self, exc_type, exc, tb):
                if exc_type is None:  # Only upload if no exception
                    self.close()
        
        return _S3Writer(uri)
    
    def reader(self, uri: str):
        """
        Context manager for reading checkpoint from storage.
        
        Args:
            uri: Full URI to checkpoint (e.g., "s3://bucket/checkpoint.pt")
        
        Returns:
            File-like reader object compatible with torch.load()
        
        Example:
            >>> with checkpoint.reader("s3://bucket/model.ckpt") as reader:
            ...     state_dict = torch.load(reader)
        """
        import io
        from .. import _pymod
        
        # Download checkpoint data
        class _S3Reader:
            def __init__(self, uri):
                self.uri = uri
                self.buffer = None
            
            def __enter__(self):
                # Download on enter
                data = _pymod.get(self.uri)
                self.buffer = io.BytesIO(bytes(data))
                return self.buffer
            
            def __exit__(self, exc_type, exc, tb):
                if self.buffer:
                    self.buffer.close()
        
        return _S3Reader(uri)


class S3ClientConfig:
    """
    Configuration for S3Client, matching s3torchconnector API.
    
    Args:
        force_path_style (bool): Force path-style S3 URLs (required for MinIO)
        max_attempts (int): Maximum retry attempts (default: 5)
    """
    def __init__(self, force_path_style=False, max_attempts=5):
        self.force_path_style = force_path_style
        self.max_attempts = max_attempts


class S3Client:
    """
    Low-level S3 client matching s3torchconnector._s3client.S3Client API.
    
    This provides compatibility for DLIO's s3_torch_storage.py which uses
    the low-level S3Client interface instead of the high-level Dataset classes.
    
    Args:
        region (str): AWS region
        endpoint (str): Custom endpoint URL (for MinIO, Ceph, etc.)
        s3client_config (S3ClientConfig): Client configuration
    """
    
    def __init__(self, region=None, endpoint=None, s3client_config=None):
        self.region = region or os.environ.get("AWS_REGION", "us-east-1")
        self.endpoint = endpoint or os.environ.get("AWS_ENDPOINT_URL")
        self.config = s3client_config or S3ClientConfig()
        
        # Import s3dlio for backend operations
        from .. import _pymod
        self._pymod = _pymod
    
    def put_object(self, bucket, key):
        """
        Create a writer for uploading object data.
        
        Args:
            bucket (str): Bucket name
            key (str): Object key (can be full s3:// URI)
        
        Returns:
            Writer object with write() and close() methods
        """
        # Parse key if it's a full URI
        if key.startswith("s3://"):
            uri = key
        else:
            uri = f"s3://{bucket}/{key.lstrip('/')}"
        
        class _S3Writer:
            def __init__(self, uri, pymod):
                self.uri = uri
                self._pymod = pymod
                self.buffer = []
            
            def write(self, data):
                """Accumulate data to write"""
                if isinstance(data, bytes):
                    self.buffer.append(data)
                else:
                    self.buffer.append(bytes(data))
            
            def close(self):
                """Upload accumulated data"""
                if self.buffer:
                    payload = b''.join(self.buffer)
                    self._pymod.put(self.uri, payload)
                    self.buffer = []
        
        return _S3Writer(uri, self._pymod)
    
    def get_object(self, bucket, key, start=None, end=None):
        """
        Create a reader for downloading object data.
        
        Args:
            bucket (str): Bucket name  
            key (str): Object key (can be full s3:// URI)
            start (int, optional): Range start byte
            end (int, optional): Range end byte (inclusive)
        
        Returns:
            Reader object with read() method
        """
        # Parse key if it's a full URI
        if key.startswith("s3://"):
            uri = key
        else:
            uri = f"s3://{bucket}/{key.lstrip('/')}"
        
        class _S3Reader:
            def __init__(self, uri, pymod, start, end):
                self.uri = uri
                self._pymod = pymod
                self.start = start
                self.end = end
                self._data = None
            
            def read(self):
                """Read object data (with optional range) - ZERO-COPY"""
                if self._data is None:
                    # Get full object (returns BytesView - zero-copy!)
                    data = self._pymod.get(self.uri)
                    
                    # Apply range if specified
                    if self.start is not None and self.end is not None:
                        # end is inclusive in s3torchconnector API
                        # Slicing BytesView creates a new BytesView (zero-copy)
                        self._data = bytes(data[self.start:self.end + 1])
                    else:
                        # âœ… ZERO-COPY: Keep BytesView (don't convert to bytes!)
                        self._data = data
                
                return self._data
        
        return _S3Reader(uri, self._pymod, start, end)
    
    def list_objects(self, bucket, prefix=""):
        """
        List objects in bucket with optional prefix.
        
        Args:
            bucket (str): Bucket name
            prefix (str): Key prefix (can be full s3:// URI)
        
        Yields:
            ListObjectResult with object_info list
        """
        # Parse prefix if it's a full URI
        if prefix.startswith("s3://"):
            uri = prefix.rstrip('/') + '/'
        else:
            uri = f"s3://{bucket}/{prefix.lstrip('/')}"
        
        # List objects using s3dlio
        from .. import list_prefix
        
        class ObjectInfo:
            """Object metadata"""
            def __init__(self, key):
                self.key = key
        
        class ListObjectResult:
            """Result from list_objects"""
            def __init__(self, object_infos):
                self.object_info = object_infos
        
        try:
            # List all objects with this prefix
            objects = list_prefix(uri)
            
            # Convert to ObjectInfo objects
            object_infos = [ObjectInfo(obj) for obj in objects]
            
            # Yield as a single result (s3torchconnector returns iterator of results)
            yield ListObjectResult(object_infos)
        except Exception as e:
            # Return empty result on error
            yield ListObjectResult([])


# Expose API-compatible classes
__all__ = [
    "S3IterableDataset",
    "S3MapDataset", 
    "S3Checkpoint",
    "S3Item",
    "S3Client",
    "S3ClientConfig",
]
